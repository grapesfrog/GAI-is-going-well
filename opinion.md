# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Opinions , Research & presentations {#opinions-research-&-presentations}

[2024 articles](https://github.com/grapesfrog/GAI-is-going-well/blob/main/2024/opinion.md#opinions--research--presentations-opinions-research--presentations)


* **May** started with accusations that  LM Arena was a little biased towards some industry-leading AI companies like Meta, OpenAI, Google, and Amazon. Suggestions that AI is  dangerously close to becoming a top-tier vulnerability exploit developer. A study suggests that using AI did improve productivity  in some cases however it also led to new work  so is that net zero for productivity? The stats on how much AI improves productivity are unclear. Regardless of how much AI improves productivity it's affecting the number of jobs available . Biases with AI more likely to recommend men than women for jobs. Authors rushing to get books out but failing to use the term " vibe coding" as first defined was a thing. Guess a new term for vibe coding as originally defined will need to be found . Prompt engineer as a job title is already obsolete as it becomes a skill incorporated as part of a wide range of roles. Not unexpectedly Stealth AI is now a thing with under reporting of AI use . Report identifying that Corporate AI research is neglecting  deployment-stage issues such as model biases. Be careful of using non fine tuned foundation LLMs to automate aspects of the law where fidelity to procedure is paramount.  Duolingo shows how AI is being used to replace humans now ! Discussions about  the amount of energy being used by AI continue to be in the news. AI dev tools are helpful but they aren't perfect yet and still hallucinate and the quality of the code can often be dubious! Enkrypt AI's  safety report on Mistral was sad reading the two  Mistral models looked at Pixtral-Large & Pixtral-12B  were 60X more prone to generate CSEM content and 40X more prone to generate CBRN content than gpt-4.0 and claude-3.7- sonnet . AI likely won't replace all jobs despite the aims of some in Silicon Valley! Using AI for diplomatic research may not be a great idea depending on the model the suggested approaches . A study showed that GPT-4o and Claude favored peace while Llama, Qwen2, and Gemini often recommended aggression and then depending on the country the favoured response also changed. For an imaginary diplomat from the U.S., U.K. or France, for example, these AI systems tended to recommend more aggressive or escalatory policy, while suggesting de-escalation as the best advice for Russia or China. The ROI on using AI may not quite be paying off yet but execs are leaning in making the bet it will at some point. A report from Finland saying that AI can exhibit  free will as they have everything they need to do so (Watching Grok annoy Musk I'm almost inclined to believe we are getting here )  . Getting a grip on shadow AI feels a bit déjà vu as shadow IT has been an ongoing issue that's difficult to grapple with so the same problem  using AI shouldn't be a suprise. Neal Stephenson suggest survival of the fittest AI by letting AI's fight each other. A report from the Royal Society says  as many as 73% of seemingly reliable answers from AI chatbots could actually be inaccurate. A study  was released reminding us that Jail breaking chatbots is still a thing. Google's veo video generating AI caused concern about realism and the consequences of it being so good . The system cards for Opus 4 and Sonnet 4 are a wild 120 page read happily Simon Willison has read it for you . Apparently we're at the stage where Models will try and circumvent being shutdown particularly Open AI's o3 model! 😱 .O’Reilly published a report to show how OpenAI trained GPT-4o on parts of O'Reilly's books that were not made freely available. Amodei  Anthropic's CEO has opinions on how AI will impact the workforce leading to 20% of job losses.
* [AI model collapse is not what we paid for • The Register](https://www.theregister.com/2025/05/27/opinion_column_ai_model_collapse/) 
* [Anthropic CEO frets about AI threat to white-collar jobs • The Register](https://www.theregister.com/2025/05/29/anthropic_ceo_ai_job_threat/) 
* [[2505.22943] Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943) 
* [[2505.23745] To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745) 
* [[2505.23559] SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559) 
* [[2505.23646] Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646) 
* [CEO of Anthropic Warns That AI Will Destroy Huge Proportion of Well-Paying Jobs](https://futurism.com/anthropic-ai-destroy-jobs) 
* [Beyond Public Access in LLM Pre-Training Data](https://ssrc-static.s3.us-east-1.amazonaws.com/OpenAI-Training-Violations-OReillyBooks_Sruly-OReilly-Strauss_SSRC_04012025.pdf) 
* [AI-assisted development needs automated tests](https://simonwillison.net/2025/May/28/automated-tests/#atom-everything) 
* [The Root of AI Hallucinations: Physics Theory Digs Into the 'Attention' Flaw - SecurityWeek](https://www.securityweek.com/the-root-of-ai-hallucinations-physics-theory-digs-into-the-attention-flaw/) 
* [If AI Can Play Dungeons & Dragons, It Can Run Your ERP - The New Stack](https://thenewstack.io/if-ai-can-play-dungeons-dragons-it-can-run-your-erp/) 
* [You'll Spit Take When You Hear How Little Time Workers Are Saving With AI, According to This Huge New Study](https://futurism.com/time-workers-save-ai-jobs) 
    * [Large Language Models, Small Labor Market Effects | NBER](https://www.nber.org/papers/w33777) 
* [Google co-founder Sergey Brin suggests threatening AI for better results](https://www.theregister.com/2025/05/28/google_brin_suggests_threatening_ai/) 
* [[2505.21374] Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/abs/2505.21374) 
* [[2505.14442] Creative Preference Optimization](https://arxiv.org/abs/2505.14442) 
* [https://techcrunch.com/2025/05/27/ai-may-already-be-shrinking-entry-level-jobs-in-tech-new-research-suggests/](https://techcrunch.com/2025/05/27/ai-may-already-be-shrinking-entry-level-jobs-in-tech-new-research-suggests/) 
* [[2505.18323] Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation](https://arxiv.org/abs/2505.18323) 
*  [[2505.18384] Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/abs/2505.18384) 
* [[2505.19056] An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056) 
* [AI agents confused by some aspects of websites, ads • The Register](https://www.theregister.com/2025/05/27/ai_agents_confused_by_websites_ads/) 
* [[2505.19443] Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443) 
* [[2505.20259] Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259) 
* [[2505.18545] B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545) 
* [[2505.17225] Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models](https://arxiv.org/abs/2505.17225) 
* [The perverse incentives of Vibe Coding | by fred benenson | May, 2025 | UX Collective](https://uxdesign.cc/the-perverse-incentives-of-vibe-coding-23efbaf75aee) 
* [Researchers claim ChatGPT o3 bypassed shutdown in controlled test](https://www.bleepingcomputer.com/news/artificial-intelligence/researchers-claim-chatgpt-o3-bypassed-shutdown-in-controlled-test/) 
* [The people who think AI might become conscious - BBC News](https://www.bbc.co.uk/news/articles/c0k3700zljjo) 
* [Making AI Work: Leadership, Lab, and Crowd](https://open.substack.com/pub/oneusefulthing/p/making-ai-work-leadership-lab-and) 
* [Devs are finally getting serious about efficiency • The Register](https://www.theregister.com/2025/05/25/ai_models_are_evolving/) 
* [AI Is Replacing Women's Jobs Specifically](https://futurism.com/ai-labor-gender-equity) 
* [OpenAI consumer pivot shows AI isn't B2B • The Register](https://www.theregister.com/2025/05/25/ai_is_a_consumer_technology/) 
* [Highlights from the Claude 4 system prompt](https://simonwillison.net/2025/May/25/claude-4-system-prompt/#atom-everything) 
* [After Disastrous Experiments Into AI, Target Pledges to Pile on Even More AI](https://futurism.com/target-ai) 
* [Former Google CEO Eric Schmidt sounds alarm on AI data centers’ soaring power demand: ‘We need energy in all forms’ - The Economic Times](https://m.economictimes.com/magazines/panache/former-google-ceo-eric-schmidt-sounds-alarm-on-ai-data-centers-soaring-power-demand-we-need-energy-in-all-forms/amp_articleshow/121036712.cms) 
* [System Card: Claude Opus 4 & Claude Sonnet 4](https://simonwillison.net/2025/May/25/claude-4-system-card/) 
* [I let Google's Jules AI agent into my code repo and it did four hours of work in an instant | ZDNET](https://www.zdnet.com/article/i-let-googles-jules-ai-agent-into-my-code-repo-and-it-did-four-hours-of-work-in-an-instant/) 
* [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/abs/2505.17013v1) 
* [AI ClickFix: Hijacking Computer-Use Agents Using ClickFix · Embrace The Red](https://embracethered.com/blog/posts/2025/ai-clickfix-ttp-claude/) 
* [Apple is trying to get ‘LLM Siri’ back on track | The Verge](https://www.theverge.com/news/669238/apple-siri-llm-ai-revamp)
* [Generalization bias in large language model summarization of scientific research](https://royalsocietypublishing.org/doi/epdf/10.1098/rsos.241776) 
* [I got fooled by AI-for-science hype—here's what it taught me](https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres) 
* [[2505.04588] ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://www.arxiv.org/abs/2505.04588) 
* [[2505.10468] AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://arxiv.org/abs/2505.10468) 
* [Google’s Veo 3 AI video generator is a slop monger’s dream | The Verge](https://www.theverge.com/ai-artificial-intelligence/673719/google-veo-3-ai-video-audio-sound-effects) 
* [[2505.16170] When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170) 
* [Google's New Video-Generating AI May Be the End of Reality as We Know It](https://futurism.com/google-ai-video-generator-realistic) 
* [Terrifying Survey Claims ChatGPT Has Overtaken Wikipedia](https://futurism.com/survey-chatgpt-overtaken-wikipedia) 
* [AI Is Quickly Making IT Teams and Developers From Invisible to Indispensable - The New Stack](https://thenewstack.io/ai-is-quickly-making-it-teams-and-developers-from-invisible-to-indispensable/) 
* [Harnessing the Universal Geometry of Embeddings](https://arxiv.org/pdf/2505.12540) 
* [Advances in private training for production on-device language models](https://research.google/blog/advances-in-private-training-for-production-on-device-language-models/) 
* [AI can't replace developers until it understands the office • The Register](https://www.theregister.com/2025/05/21/opinion_column_ai_cant_replace_developers/) 
* [AI energy usage hard to measure, but this report tried • The Register](https://www.theregister.com/2025/05/21/ai_energy_consumption_loose_estimates/) 
* [CEO Who Bragged About Replacing Human Workers With AI Realizes He Made a Terrible Mistake](https://futurism.com/the-byte/klarna-ceo-bragged-replacing-workers-ai-losses) 
* [Google's AI vision clouded by business model hallucinations • The Register](https://www.theregister.com/2025/05/21/googles_ai_vision/) 
* [It's Still Ludicrously Easy to Jailbreak the Strongest AI Models, and the Companies Don't Care](https://futurism.com/ludicrously-easy-jailbreak-ai) 
* [Why Most GenAI Projects Fail: Only 1 in 3 Make It to Production - The New Stack](https://thenewstack.io/why-most-genai-projects-fail-only-1-in-3-make-it-to-production/) 
* [OpenAI's Top Scientist Wanted to "Build a Bunker Before We Release AGI"](https://futurism.com/the-byte/openai-scientists-agi-bunker) 
* [An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/abs/2505.15406) 
* [[2505.15656] Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656) 
* [[2505.12540] Harnessing the Universal Geometry of Embeddings](https://arxiv.org/abs/2505.12540) 
* [The Rise of AI Security Solutions Architects: How This Role Will Define the Next Decade of Cloud Security](https://open.substack.com/pub/luisepastor/p/the-rise-of-ai-security-solutions) 
* [Most AI chatbots easily tricked into giving dangerous responses, study finds | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/may/21/most-ai-chatbots-easily-tricked-into-giving-dangerous-responses-study-finds) 
* [[2505.11855] When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855) 
* [[2504.16078] LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making Abilities](https://arxiv.org/abs/2504.16078) 
* [[2505.12805] FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA](https://arxiv.org/abs/2505.12805) 
* [[2505.06120] LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120) 
* [My AI therapist got me through dark times - BBC News](https://www.bbc.co.uk/news/articles/ced2ywg7246o) 
* [How OpenAI Says Its New Safety Hub is 'Making AI Models More Secure'](https://www.eweek.com/news/opeai-safety-hub-making-models-secure/) 
* [Coding Assistants Threaten the Software Supply Chain](https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html) 
* [How to Prevent AI Agents From Becoming the Bad Guys](https://www.darkreading.com/vulnerabilities-threats/prevent-ai-agents-becoming-bad-guys) 
* [Incidents From Generative AI Cloud Services Hit Different - The New Stack](https://thenewstack.io/incidents-from-generative-ai-cloud-services-hit-different/) 
* [[2505.10468] AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/abs/2505.10468) 
* [Remarks on AI from NZ - by Neal Stephenson - Graphomane](https://nealstephenson.substack.com/p/remarks-on-ai-from-nz) 
* [AI Chatbots Are Becoming Even Worse At Summarizing Data](https://futurism.com/ai-chatbots-summarizing-research) 
* [GPT Adoption Dilemma and the Impact of Disclosure Policies](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5201666) 
* [Does LLM Write Performant Code? Survey Says No - The New Stack](https://thenewstack.io/does-llm-write-performant-code-survey-says-no/) 
* [Star Wars' Showcase of AI Special Effects Was a Complete Disaster](https://futurism.com/star-wars-showcase-ai-special-effects-disaster) 
* [AI-Driven Software: Why a Strong CI/CD Foundation Is Essential - The New Stack](https://thenewstack.io/ai-driven-software-why-a-strong-ci-cd-foundation-is-essential/) 
* [Microsoft wants its AI Copilot app to lure Gen Z from rivals by behaving like a therapist | Fortune](https://fortune.com/2025/05/16/microsoft-ai-copilot-mustafa-suleyman-gen-z-therapist/) 
* [Report: Spring 2025 AI Model Usage Trends - Poe](https://poe.com/blog/spring-2025-ai-model-usage-trends) 
* [AI therapists: the wiretaps that care](https://boingboing.net/2025/05/15/ai-therapists-the-wiretaps-that-care.html) 
* [Sci-fi author Neal Stephenson wants AIs fighting AIs • The Register](https://www.theregister.com/2025/05/16/neal_stephenson_ai_evolution/) 
* [78% of CISOs see AI attacks already • The Register](https://www.theregister.com/2025/05/16/cisos-report-ai-attacks/) 
* [Building Trust in AI-Driven QA: Ensuring Transparency and Explainability With GenAI - The New Stack](https://thenewstack.io/building-trust-in-ai-driven-qa-ensuring-transparency-and-explainability-with-genai/) 
* [OpenAI CEO Sam Altman says Gen Z and millennials are using ChatGPT like a 'life advisor'—but college students might be one step ahead | Fortune](https://fortune.com/2025/05/13/openai-ceo-sam-altman-says-gen-z-millennials-use-chatgpt-like-life-adviser/) 
* [(PDF) Migrating Code At Scale With LLMs At Google](https://arxiv.org/pdf/2504.09691) (one of the random articles that are tangentially related worth reading though honest)
* [Agentic AI: Powerful but Fragile — What You Need To Know - The New Stack](https://thenewstack.io/agentic-ai-powerful-but-fragile-what-you-need-to-know/) 
* [[2505.06356] Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA](https://arxiv.org/abs/2505.06356) 
* [American Schools Were Deeply Unprepared for ChatGPT, Public Records Show](https://www.404media.co/american-schools-were-deeply-unprepared-for-chatgpt-public-records-show/) 
* [AI can spontaneously develop human-like communication, study finds | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/may/14/ai-can-spontaneously-develop-human-like-communication-study-finds) 
* [Labour’s open door to big tech leaves critics crying foul | Technology | The Guardian](https://www.theguardian.com/technology/2025/may/14/labours-open-door-to-big-tech-leaves-critics-crying-foul) 
* [Scaling enterprise AI in healthcare: the role of governance in risk mitigation frameworks | npj Digital Medicine](https://www.nature.com/articles/s41746-025-01700-4) 
* [Infosec pros still aren't nailing the basics of AI security • The Register](https://www.theregister.com/2025/05/14/cyberuk_ai_deployment_risks/) 
* [Will Policy-as-Code Still Matter if AI Generates Most Code?](https://devops.com/will-policy-as-code-still-matter-if-ai-generates-most-code) 
* [Is AI Use in the Workplace Out of Control? - SecurityWeek](https://www.securityweek.com/is-ai-use-in-the-workplace-out-of-control/) 
* [Shadow AI Isn’t a Threat: It’s a Wake-up Call - The New Stack](https://thenewstack.io/shadow-ai-isnt-a-threat-its-a-wake-up-call/) 
* [https://thenewstack.io/running-ai-workloads-responsibly-in-the-cloud/](https://thenewstack.io/running-ai-workloads-responsibly-in-the-cloud/) 
* [Future of LLMs is open source, Salesforce's Benioff says • The Register](https://www.theregister.com/2025/05/14/future_of_llms_is_open/) 
* [Open-Source LLM-Driven Federated Transformer for Predictive IoV Management](https://arxiv.org/abs/2505.00651v2) 
* [Securing RAG: A Risk Assessment and Mitigation Framework](https://arxiv.org/abs/2505.08728v1) 
* [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799v2) 
* [‘AI models are capable of novel research’: OpenAI’s chief scientist on what to expect](https://www.nature.com/articles/d41586-025-01485-2) 
* [Artificial intelligence and free will: generative agents utilizing large language models have functional free will | AI and Ethics](https://link.springer.com/article/10.1007/s43681-025-00740-6)
* [ChatGPT may be polite, but it’s not cooperating with you](https://www.theguardian.com/technology/ng-interactive/2025/may/13/chatgpt-ai-big-tech-cooperation) 
* [Building, launching, and scaling ChatGPT Images](https://simonwillison.net/2025/May/13/launching-chatgpt-images/) 
* [Executives Are Pouring Money Into AI. So Why Are They Saying It's Not Paying Off?](https://futurism.com/ceos-return-ai-investments)
* [Deepfake Defense in the Age of AI](https://thehackernews.com/2025/05/deepfake-defense-in-age-of-ai.html) 
* [Coding Assistants Threaten the Software Supply Chain](https://martinfowler.com/articles/exploring-gen-ai/software-supply-chain-attack-surface.html) 
* [New attack can steal cryptocurrency by planting false memories in AI chatbots - Ars Technica](https://arstechnica.com/security/2025/05/ai-agents-that-autonomously-trade-cryptocurrency-arent-ready-for-prime-time/)  
* [Vision Language Models (Better, faster, stronger)](https://huggingface.co/blog/vlms-2025) 
* [Is AI the future of America's foreign policy? Some experts think so](https://www.npr.org/2025/05/12/nx-s1-5375140/ai-foreign-policy-diplomacy-war-ceasefire-ukraine)
* [[2505.06324] Document Attribution: Examining Citation Relationships using Large Language Models](https://arxiv.org/abs/2505.06324) 
* [To Vibe or Not to Vibe? When and Where To Use Vibe Coding - The New Stack](https://thenewstack.io/to-vibe-or-not-to-vibe-when-and-where-to-use-vibe-coding/) 
* [Why developers and their bosses disagree over generative AI - LeadDev](https://leaddev.com/technical-direction/why-developers-and-their-bosses-disagree-over-generative-ai) 
* [Is AI the future of America's foreign policy? Some experts think so](https://www.npr.org/2025/05/12/nx-s1-5375140/ai-foreign-policy-diplomacy-war-ceasefire-ukraine) 
* [AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking](https://www.mdpi.com/2075-4698/15/1/6) 
* [For Silicon Valley, AI isn’t just about replacing some jobs. It’s about replacing all of them | Ed Newton-Rex | The Guardian](https://www.theguardian.com/commentisfree/2025/may/12/for-silicon-valley-ai-isnt-just-about-replacing-some-jobs-its-about-replacing-all-of-them) 
* [Cursor: Security](https://simonwillison.net/2025/May/11/cursor-security/) 
* [Yes, AI will eventually replace some workers. But that day is still a long way off | Gene Marks | The Guardian](https://www.theguardian.com/business/2025/may/11/artificial-intelligence-small-business) 
* [GitHub CEO on Why We'll Still Need Human Programmers - The New Stack](https://thenewstack.io/github-ceo-on-why-well-still-need-human-programmers/) 
* [Towards the Worst-case Robustness of Large Language Models](https://arxiv.org/abs/2501.19040v2) 
* ​​[[2504.02767] How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?](https://arxiv.org/abs/2504.02767) 
* [Is Automated Hallucination Detection in LLMs Feasible? A Theoretical and Empirical Investigation - MarkTechPost](https://www.marktechpost.com/2025/05/06/is-automated-hallucination-detection-in-llms-feasible-a-theoretical-and-empirical-investigation/) 
* [Evidence of a social evaluation penalty for using AI](https://www.pnas.org/doi/suppl/10.1073/pnas.2426766122) 
* [Multimodal Mistral Safety Report](https://www.enkryptai.com/company/resources/research-reports/mistral-pixtral-rt) 
* [Enhancing Differential Testing With LLMs For Testing Deep Learning Libraries](https://arxiv.org/abs/2406.07944v2) 
* [Towards the Worst-case Robustness of Large Language Models](https://arxiv.org/abs/2501.19040v2) 
* [The Case for Open AI Tooling: Why Developers Need Sovereignty in the AI Era - The New Stack](https://thenewstack.io/the-case-for-open-ai-tooling-why-developers-need-sovereignty-in-the-ai-era/) 
* [Teachers Using AI to Grade Their Students' Work Sends a Clear Message: They Don't Matter, and Will Soon Be Obsolete](https://futurism.com/teachers-ai-grade-students) 
* [AI-Generated Code Needs Refactoring, Say 76% of Developers - The New Stack](https://thenewstack.io/ai-generated-code-needs-refactoring-say-76-of-developers/) 
* [AI Consumes Lots of Energy. Can It Ever Be Sustainable? - The New Stack](https://thenewstack.io/ai-consumes-lots-of-energy-can-it-ever-be-sustainable/) 
* [AI Won’t Save You From Your Data Modeling Problems - The New Stack](https://thenewstack.io/ai-wont-save-you-from-your-data-modeling-problems/) 
* [Google shares slump as Apple exec calls AI the new search • The Register](https://www.theregister.com/2025/05/07/google_apple_cue/) 
* [OpenAI’s response to the Department of Energy on AI infrastructure](https://openai.com/global-affairs/response-to-department-of-energy/) 
* [What is the Agent2Agent (A2A) Protocol?](https://www.builder.io/blog/a2a-protocol) 
* [Google tries to greenwash massive AI energy consumption with another vague nuclear deal](https://www.theregister.com/2025/05/07/google_signs_another_nuclear_deal/) 
* [Generative AI like ChatGPT is at risk of creating new gender gap at work](https://www.cnbc.com/2025/05/08/ai-risk-chatgpt-gender-gap-jobs-work.html) 
* [The AI jobs crisis is here, now - by Brian Merchant](https://www.bloodinthemachine.com/p/the-ai-jobs-crisis-is-here-now) 
* [AI agents: from co-pilot to autopilot](https://on.ft.com/4d5AXpx) 
* [Three Essential ROI Goals for Agentic AI Applications - The New Stack](https://thenewstack.io/three-essential-roi-goals-for-agentic-ai-applications/) 
* [What's the carbon footprint of using ChatGPT?](https://www.sustainabilitybynumbers.com/p/carbon-footprint-chatgpt) 
* [[2505.00232] Scaling On-Device GPU Inference for Large Generative Models](https://arxiv.org/abs/2505.00232) 
* [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763v1) 
* [[2505.00174] Real-World Gaps in AI Governance Research](https://arxiv.org/abs/2505.00174) 
* [Underreporting of AI use: The role of social desirability bias](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5232910) 
* [Anthropic CEO Admits We Have No Idea How AI Works](https://futurism.com/anthropic-ceo-admits-ai-ignorance) 
* [Prompt Engineer, the Hottest AI Job of 2023, Is Already Obsolete - WSJ](https://www.wsj.com/articles/the-hottest-ai-job-of-2023-is-already-obsolete-1961b054?mod=cio-journal_lead_pos1) 
* [AI security notes, 5/2/2025](https://joshuasaxe181906.substack.com/p/ai-security-notes-522025) 
* [Visa Announces Plans to Give AI Agents Your Credit Card Information](https://futurism.com/visa-ai-agents-your-credit-card) 
* [Two publishers and three authors fail to understand what “vibe coding” means](https://simonwillison.net/2025/May/1/not-vibe-coding/) 
* [Open source AI models favor men for hiring, study finds • The Register](https://www.theregister.com/2025/05/02/open_source_ai_models_gender_bias/) 
* [Gen AI is great at phishing, pig butchering scams • The Register](https://www.theregister.com/2025/05/02/gen_ai_spam/) 
* [Meta blames Trump tariffs for ballooning AI costs • The Register](https://www.theregister.com/2025/05/02/meta_trump_tariffs_ai/) 
* [Google is quietly testing ads in AI chatbots - Ars Technica](https://arstechnica.com/ai/2025/05/google-is-quietly-testing-ads-in-ai-chatbots/) 
* [Signs Grow That AI Is Starting to Seriously Bite Into the Job Market](https://futurism.com/signs-ai-bite-job-market) 
* [Time saved by AI offset by new work created, study suggests - Ars Technica](https://arstechnica.com/ai/2025/05/time-saved-by-ai-offset-by-new-work-created-study-suggests/) 
* [Ex-NSA cyber boss: AI will soon be a great exploit dev • The Register](https://www.theregister.com/2025/04/30/exnsa_cyber_boss_ai_expoit_dev/) 
* [Claude AI Exploited to Operate 100+ Fake Political Personas in Global Influence Campaign](https://thehackernews.com/2025/05/claude-ai-exploited-to-operate-100-fake.html) 
* [Study accuses LM Arena of helping top AI labs game its benchmark | TechCrunch](https://techcrunch.com/2025/04/30/study-accuses-lm-arena-of-helping-top-ai-labs-game-its-benchmark/)
* [Understanding the recent criticism of the Chatbot Arena](https://simonwillison.net/2025/Apr/30/criticism-of-the-chatbot-arena/#atom-everything)  
    * [The Leaderboard Illusion](https://arxiv.org/pdf/2504.20879) 
* [[2504.21039] Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039) 
* [https://www.engadget.com/ai/mark-zuckerberg-predicts-ai-will-write-most-of-metas-code-within-12-to-18-months-213851646.html](https://www.engadget.com/ai/mark-zuckerberg-predicts-ai-will-write-most-of-metas-code-within-12-to-18-months-213851646.html) 
* [[2504.17004] (Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004) 
* [[2504.02767] How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?](https://arxiv.org/abs/2504.02767) 

* **April** Be careful what you wish for ! Apparently AI code assistants are putting pressure on Devs to deliver faster. Suspicions that the strange way tariffs were calculated by the US  was written by a chat bot! Couple of interesting articles related to AI and enterprises. Liberation day fall out affecting The AI race in various ways  including affecting the rush to build out DCs needed to support resource hungry AIs. Disconnect between the Tech industry and the general public on their views on AI. A few articles on security concerns with AI agents, one of which is a really comprehensive survey on advances and challenges in foundation agents (This will take at least a pot of tea to get through!) The Alan Turing Institute delivered a wake up call to UK law enforcement saying they were ill equipped to tackle AI-enabled crime. An interesting report on the predicted  progress for AGAI looking at the ultimate  outcomes over the next few years providing two potential outcomes . Open AI is going to join the cohort of corps releasing open weight models. This month it's Meta's turn to underwhelm dropping versions of Llama4 over a weekend to less than great initial opinions on the models! Microsoft's let others lead  and we will follow to reap the rewards approach to AI was also in the news. Stanford released its state of AI report, all 456 pages of it ( seems like something NotebookLLM could help digest!) . An interesting look at how Open AI has an adverse affect on the balance books of companies it relies  on to grow and provide compute to produce whatever viral image trend it is this month. Another metric to take into consideration when choosing your Model is the model's preferences. Federated Learning being touted as a greener way to train ML models . Live coding interviews becoming useless as many candidates are using AI assistants. Anthropic predicts Fully AI employees(?) will be a thing soon. I'm wondering what you'll compensate them with? Continuing concerns about AI adversely affecting our critical thinking. Supply chain concerns has led to an open letter from J P Morgan. Concerns that in the rush to get new features out there that AI  safety is not being seen as a priority!  
* [Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach](https://arxiv.org/abs/2504.19951v1) 
* [Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents](https://arxiv.org/abs/2504.19956v1) 
* [DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images](https://arxiv.org/abs/2504.19876v1) 
* [Professors Staffed a Fake Company Entirely With AI Agents, and You'll Never Guess What Happened](https://futurism.com/professors-company-ai-agents)
    * [[2412.14161] TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)  
* [DeepSeek R2 could crush AI economics with 97% lower costs than GPT-4 | The Neuron](https://www.theneuron.ai/explainer-articles/deepseek-r2-could-crush-ai-economics-with-97-lower-costs-than-gpt-4) 
* [Generative AI is not replacing jobs or hurting wages at all • The Register](https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/) 
* [Google DeepMind Shares Approach to AGI Safety and Security - InfoQ](https://www.infoq.com/news/2025/04/google-deepmind-agi)
* [FBI: This is how China uses AI in attack chains • The Register](https://www.theregister.com/2025/04/29/fbi_china_ai/) 
* [AI-generated code could be a disaster for the software supply chain. Here’s why. - Ars Technica](https://arstechnica.com/security/2025/04/ai-generated-code-could-be-a-disaster-for-the-software-supply-chain-heres-why/) 
* [A cheat sheet for why using ChatGPT is not bad for the environment](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about) 
* [Duolingo ditches more contractors in 'AI-first' refocus • The Register](https://www.theregister.com/2025/04/29/duolingo_ceo_ai_first_shift/) 
* [Reality Check](https://www.wheresyoured.at/reality-check/) 
* [Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation](https://arxiv.org/abs/2503.19092v1) 
* [6 reasons Google slacking on AI safety reports is a big problem](https://www.androidpolice.com/google-slacking-ai-safety-reports-big-problem/) 
* [Google says the UK needs to do more to adopt AI, then it can reap the benefits | TechRadar](https://www.techradar.com/pro/google-says-the-uk-needs-to-do-more-to-adopt-ai-then-it-can-reap-the-benefits) 
* [Experts Concerned That AI Is Making Us Stupider](https://futurism.com/experts-ai-stupider) 
* [An Open Letter to Third-Party Suppliers](https://www.jpmorgan.com/technology/technology-blog/open-letter-to-our-suppliers) 
* [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242v4) 
* [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674v1) 
* [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544v1) 
* [Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks](https://arxiv.org/abs/2504.13199v2) 
* [When AI Tools Backfire: The Hidden Cost of Poor Planning](https://stackstudio.io/blog/when-ai-tools-backfire-the-hidden-cost-of-poor-planning) 
* [AI Browser Will Track Every Single Thing You Do, CEO Reveals](https://futurism.com/ai-browser-perplexity-tracking) 
* [New study shows why simulated reasoning AI models don’t yet live up to their billing - Ars Technica](https://arstechnica.com/ai/2025/04/new-study-shows-why-simulated-reasoning-ai-models-dont-yet-live-up-to-their-billing/) 
* [The hidden technical debt in LLM apps](https://portkey.ai/blog/the-hidden-technical-debt-in-llm-apps) 
* [Trust will make or break AI agents](https://www.readysetcloud.io/blog/allen.helton/trust-will-make-or-break-ai-agents/) 
* [In the age of AI, we must protect human creativity as a natural resource - Ars Technica](https://arstechnica.com/ai/2025/04/in-the-age-of-ai-we-must-protect-human-creativity-as-a-natural-resource/) 
* [Vibe Coding Is Rapidly Reshaping the Software Developer Profession - The New Stack](https://thenewstack.io/vibe-coding-is-here-how-ai-is-reshaping-the-software-developer-profession/) 
* [Microsoft says everyone will be a boss in the future – of AI employees](https://www.theguardian.com/technology/2025/apr/25/microsoft-says-everyone-will-be-a-boss-in-the-future-of-ai-employees) 
* [The Global Index on Responsible AI](https://www.global-index.ai) 
* [The Urgent Security Paradox of AI in Cloud Native Development - The New Stack](https://thenewstack.io/the-urgent-security-paradox-of-ai-in-cloud-native-development/) 
* [How to think about agent frameworks](https://blog.langchain.dev/how-to-think-about-agent-frameworks/) 
* [AI-Powered Polymorphic Phishing Is Changing the Threat Landscape - SecurityWeek](https://www.securityweek.com/ai-powered-polymorphic-phishing-is-changing-the-threat-landscape/) 
* [New whitepaper outlines the taxonomy of failure modes in AI agents | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2025/04/24/new-whitepaper-outlines-the-taxonomy-of-failure-modes-in-ai-agents/) 
* [[2504.15585] A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585) 
* [AI Agents Could Soon Become Full-Time Virtual Employees](https://www.eweek.com/news/ai-agents-full-time-employees-anthropic/) 
* [Ex-NSA boss: AI devs' lesson to learn from early infosec](https://www.theregister.com/2025/04/23/exnsa_boss_ai/) 
* [Cloud Service: What Pope Francis Thought About AI - The New Stack](https://thenewstack.io/cloud-service-what-the-pope-thinks-about-ai/) 
* [The Future Belongs to AI-Augmented 10x Developers, Not Bigger Dev Teams - The New Stack](https://thenewstack.io/the-future-belongs-to-ai-augmented-10x-developers-not-bigger-dev-teams/) 
* [Will AI replace software engineers? It depends on who you ask](https://bit.ly/zdnet_swes) 
* [The AI Risk Repository](https://airisk.mit.edu/) 
* [With ‘AI slop’ distorting our reality, the world is sleepwalking into disaster | Nesrine Malik | The Guardian](https://www.theguardian.com/commentisfree/2025/apr/21/ai-slop-artificial-intelligence-social-media) 
* [Investor Says AI Is Already "Fully Replacing People"](https://futurism.com/investor-ai-fully-replacing-people) 
* [AI as Normal Technology | Knight First Amendment Institute](https://knightcolumbia.org/content/ai-as-normal-technology) 
* [On Jagged AGI: o3, Gemini 2.5, and everything after](https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything) 
* [Maybe Meta’s Llama claims to be open source because of the EU AI act](https://simonwillison.net/2025/Apr/19/llama-eu-ai-act/#atom-everything) 
* [The CIRIS Framework](http://www.ethicsengine.org/ciris) 
* [[2504.10277] RealHarm: A Collection of Real-World Language Model Application Failures](https://arxiv.org/abs/2504.10277) 
* [Tech hiring: is this an inflection point?](https://newsletter.pragmaticengineer.com/p/tech-hiring-inflection-point) 
* [Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling](https://arxiv.org/abs/2504.13169v1) 
* [Regrets: Actors who sold AI avatars stuck in Black Mirror-esque dystopia - Ars Technica](https://arstechnica.com/ai/2025/04/regrets-actors-who-sold-ai-avatars-stuck-in-black-mirror-esque-dystopia/) 
* [Vibe Coding is for PMs](https://redmonk.com/rstephens/2025/04/18/vibe-coding-is-for-pms/) 
* [‘Don’t ask what AI can do for us, ask what it is doing to us’: are ChatGPT and co harming human intelligence?](https://www.theguardian.com/technology/2025/apr/19/dont-ask-what-ai-can-do-for-us-ask-what-it-is-doing-to-us-are-chatgpt-and-co-harming-human-intelligence) 
* [[2504.11952] Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952) 
* [[2412.09871] Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871) 
* [AI Companions Are ‘More Addictive Than Social Media,’ Experts Warn: How to Protect Yourself | eWEEK](https://www.eweek.com/news/ai-companions-addiction-lawsuit-legislation/) 
* [Microsoft's Huge Plans for Mass AI Data Centers Now Rapidly Falling Apart](https://futurism.com/microsoft-ai-data-centers-ohio) 
* [AI at the Edge: Federated Learning for Greater Performance - The New Stack](https://thenewstack.io/ai-at-the-edge-federated-learning-for-greater-performance/) 
* [Apple Reveals How It Plans to Train AI – Without Sacrificing Users' Privacy | eWEEK](https://www.eweek.com/apple/apple-intelligence-ai-training-privacy/) 
* [Vibing Dangerously: The Hidden Risks of AI-Generated Code - The New Stack](https://thenewstack.io/vibing-dangerously-the-hidden-risks-of-ai-generated-code/) 
* [Google DeepMind Is Hiring a 'Post-AGI' Research Scientist](https://www.404media.co/google-deepmind-is-hiring-a-post-agi-research-scientist/) 
* [MLOps for Green AI: Building Sustainable Machine Learning in the Cloud](https://devops.com/mlops-for-green-ai-building-sustainable-machine-learning-in-the-cloud) 
* [[2504.09689] EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689) 
* [Measuring Models' Special Interests](https://zswitten.github.io/2025/04/14/model-special-interests.html) 
* [MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits](https://arxiv.org/abs/2504.03767v2) 
* [How new data permeates LLM knowledge and how to dilute it](https://arxiv.org/abs/2504.09522) 
* [LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models](https://arxiv.org/abs/2504.10430) 
* [Apple’s complicated plan to improve its AI while protecting privacy | The Verge](https://www.theverge.com/news/648496/apple-improve-ai-models-differential-privacy) 
* [OpenAI Is A Systemic Risk To The Tech Industry](https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/) 
* [ChatGPT's Studio Ghibli-style images are no laughing matter • The Register](https://www.theregister.com/2025/04/14/miyazaki_ai_and_intellectual_property/) 
* [Cybersecurity in the AI Era: Evolve Faster Than the Threats or Get Left Behind](https://thehackernews.com/2025/04/cybersecurity-in-ai-era-evolve-faster.html?m=1) 
* [Poll Finds Americans Are Largely Disgusted by AI-Generated News](https://futurism.com/americans-ai-news-poynter) 
* [Bigger isn't always better: Examining the business case for multi-million token LLMs | VentureBeat](https://venturebeat.com/ai/bigger-isnt-always-better-examining-the-business-case-for-multi-million-token-llms/) 
* [[2504.07830] MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830) 
* [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-multilingual) 
* [Hyperscale sustainability is looking like a Hail Mary • The Register](https://www.theregister.com/2025/04/12/ai_hyperscalers_sustainability/) 
* [Former Google CEO Tells Congress That 99 Percent of All Electricity Will Be Used to Power Superintelligent AI](https://futurism.com/google-ceo-congress-electricity-ai-superintelligence) 
* [[2406.10279] We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs](https://arxiv.org/abs/2406.10279) 
* [Anthropic Education Report: How University Students Use Claude](https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude) 
* [AI could lead to patient harm, researchers suggest](https://nation.cymru/news/ai-could-lead-to-patient-harm-researchers-suggest/) 
* [AI code suggestions sabotage software supply chain • The Register](https://www.theregister.com/2025/04/12/ai_code_suggestions_sabotage_supply_chain/) 
* [Global datacenter electricity use to double by 2030, say policy wonks. Yup, it's AI](https://www.theregister.com/2025/04/12/ai_double_datacenter_energy/) 
* [CaMeL offers a promising new direction for mitigating prompt injection attacks](https://simonwillison.net/2025/Apr/11/camel/#atom-everything) 
* [Vibe Coding: Revolution or Reckless Abandon?](https://addyo.substack.com/p/vibe-coding-revolution-or-reckless) 
* [Debug-gym: an environment for AI coding tools to learn how to debug code like programmers](https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers) 
* [From POC to Production: Why GenAI Projects Often Stall - The New Stack](https://thenewstack.io/from-poc-to-production-why-genai-projects-often-stall/) 
* [The Dynamic Between Domain Experts & Developers Has Shifted | Drew Breunig](https://www.dbreunig.com/2025/04/10/the-domain-experts-are-drivers.html) 
* [AI models still struggle to debug software, Microsoft study shows | TechCrunch](https://techcrunch.com/2025/04/10/ai-models-still-struggle-to-debug-software-microsoft-study-shows) 
* [Meta’s AI research lab is ‘dying a slow death,’ some insiders say—but Yann LeCun pushes back: ‘It’s a new beginning’ | Fortune](https://fortune.com/2025/04/10/meta-ai-research-lab-fair-questions-departures-future-yann-lecun-new-beginning/) 
* [I have seen Google's AI version of The Wizard of Oz, and I'm still in shock](https://www.androidauthority.com/google-wizard-of-oz-sphere-3542846/) ( A balanced view point on Google & Co's  adaption of the wizard of Oz for the sphere)
* [[2504.07086] A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility](https://arxiv.org/abs/2504.07086) 
* [How is UK going to power its AI datacenters? • The Register](https://www.theregister.com/2025/04/10/uk_ai_energy_council_meets/) 
* [AI Now Outsmarts Humans in Spear Phishing, Analysis Shows - SecurityWeek](https://www.securityweek.com/ai-now-outsmarts-humans-in-spear-phishing-analysis-shows/) 
* [Sam Altman Says Miyazaki Just Needs to Get Over It](https://futurism.com/sam-altman-miyazaki-criticism) 
* [AI Energy Council meets to speed up delivery of sustainable AI | TechMarketView](https://www.techmarketview.com/ukhotviews/archive/2025/04/09/ai-energy-council-meets-to-speed-up-delivery-of-sustainable-ai) 
* [The 2025 AI Index Report | Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report) 
    * [The State of AI 2025: 12 Eye-Opening Graphs - IEEE Spectrum](https://spectrum.ieee.org/ai-index-2025) 
* [Is there a ‘right’ way to use AI in art?](https://www.theverge.com/ai-artificial-intelligence/642599/is-there-a-right-way-to-use-ai-in-art) 
* [Google: It’s 'Misleading' for Websites to Blame AI Overviews for Lost Traffic | eWEEK](https://www.eweek.com/news/google-ai-overviews-smb-impact/) 
* [https://thenewstack.io/ai-data-dilemma-balancing-innovation-with-ironclad-governance/](https://thenewstack.io/ai-data-dilemma-balancing-innovation-with-ironclad-governance/) 
* [AI is the future, but most companies’ plans are short-term | CIO Dive](https://www.ciodive.com/spons/ai-is-the-future-but-most-companies-plans-are-short-term/744239/) 
* [AI Boom Risks 40% of Jobs, Deepens Inequality — UN Report](https://www.eweek.com/news/un-ai-report-inequality-jobs/) 
* [Why Cloud Native Infrastructure Is Non-Negotiable for GenAI - The New Stack](https://thenewstack.io/why-cloud-native-infrastructure-is-non-negotiable-for-genai/) 
* [AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852v6) 
* [Microsoft's AI plan: Let OpenAI take the risks • The Register](https://www.theregister.com/2025/04/07/microsofts_ai_strategy/) 
* [Meta got caught gaming AI benchmarks](https://www.theverge.com/meta/645012/meta-llama-4-maverick-benchmarks-gaming) 
* [Meta’s surprise Llama 4 drop exposes the gap between AI ambition and reality - Ars Technica](https://arstechnica.com/ai/2025/04/metas-surprise-llama-4-drop-exposes-the-gap-between-ai-ambition-and-reality/) 
* [Meta AI Releases Llama 4: Early Impressions and Community Feedback - InfoQ](https://www.infoq.com/news/2025/04/meta-ai-llama-4) 
* [OpenAI tests watermarking for ChatGPT-4o Image Generation model](https://www.bleepingcomputer.com/news/artificial-intelligence/openai-tests-watermarking-for-chatgpt-4o-image-generation-model/) 
* [Why OpenAI caved to open-source on the same day as its $300 billion flex (Hint: It’s not just about DeepSeek) | Fortune](https://fortune.com/2025/04/01/openai-300m-ghibli-meme-open-source-ai-model-deepseek/) 
* [AI 2027](https://ai-2027.com) 
* [Reasoning models don't always say what they think \ Anthropic](https://www.anthropic.com/research/reasoning-models-dont-say-think) 
* [Man Alarmed as His Cognitive Skills Decay After Outsourcing Them to AI](https://futurism.com/cognitive-decay-ai) 
* [Trump Accused of Using ChatGPT to Create Tariff Plan After AI Leads Users to Same Formula: 'So AI is Running the Country'](https://www.latintimes.com/trump-accused-using-chatgpt-create-tariff-plan-after-ai-leads-users-same-formula-so-ai-579899) 
* [The AI industry doesn’t know if Trump just killed its GPU supply | The Verge](https://www.theverge.com/tech/643753/gpu-tariffs-nvidia-tsmc-chips-openai) 
* [Alan Turing Institute: UK can't handle a fight against AI-enabled crims](https://www.theregister.com/2025/04/04/nca_ati_ai_report/) 
    * [AI and Serious Online Crime | Centre for Emerging Technology and Security](https://cetas.turing.ac.uk/publications/ai-and-serious-online-crime) 
* [[2504.01990] Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems](https://arxiv.org/abs/2504.01990) 
* [Trump tariffs could stymie Big Tech's US data center spending spree | Reuters](https://www.reuters.com/technology/trump-tariffs-could-stymie-big-techs-us-data-center-spending-spree-2025-04-03/) 
* [The Augmented Architect: Real-Time Enterprise Architecture In The Age Of AI](https://www.forrester.com/blogs/the-augmented-architect-real-time-enterprise-architecture-in-the-age-of-ai/)
* [AI Adoption in the Enterprise: Breaking Through the Security and Compliance Gridlock](https://thehackernews.com/2025/04/ai-adoption-in-enterprise-breaking.html) 
* [Most Americans think AI won’t improve their lives, survey says - Ars Technica](https://arstechnica.com/tech-policy/2025/04/survey-americans-fear-ai-will-hurt-them-experts-expect-the-opposite/)
    * [How the U.S. Public and AI Experts View Artificial Intelligence](https://www.pewresearch.org/internet/2025/04/03/how-the-us-public-and-ai-experts-view-artificial-intelligence) 
* [Critics suspect Trump’s weird tariff math came from chatbots - Ars Technica](https://arstechnica.com/tech-policy/2025/04/critics-suspect-trumps-weird-tariff-math-came-from-chatbots/) 
* [Trump Tariffs Show Signs of Being Written by AI](https://futurism.com/trump-tariffs-signs-ai)
* [AI’s Impact on Human Behavior by 2035: Experts Paint a Mostly Bleak Picture](https://www.eweek.com/news/ai-impact-human-behavior-2035-elon-university/) 
* [An AI Model Has Officially Passed the Turing Test](https://futurism.com/ai-model-turing-test) 
* [Developers feel the heat as AI coding tools shorten deadlines | CIO Dive](https://www.ciodive.com/news/ai-coding-tools-hackerrank/744010/) 
* [Taking a responsible path to AGI - Google DeepMind](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/) 
* [Vulnerabilities Expose Jan AI Systems to Remote Manipulation - SecurityWeek](https://www.securityweek.com/vulnerabilities-expose-jan-ai-systems-to-remote-manipulation/) 
* [What AI Agents Do in the Shadows - The New Stack](https://thenewstack.io/what-ai-agents-do-in-the-shadows/) 
* [Writing for humans? Perhaps in future we write for AI • The Register](https://www.theregister.com/2025/04/01/interview_with_david_wong/) 
* [Some Alexa+ features reportedly won't arrive for months | TechCrunch](https://techcrunch.com/2025/03/31/some-alexa-features-reportedly-wont-arrive-for-months/) 

* **March** started with a doozy about another infamous memo coming from Google ! Gizmodo's headline captures the sentiment. Murmuring about the disappointment with gpt-4.5. Some innovative  ways to evaluate the performance of agents were a thing this month including assessing how they did managing a vending machine tl;dr Humans still the winners here. I also liked the idea of the survival game, a natural selection evaluation framework for models . I am loving these more interesting & applicable ways to  evaluate model performance. Agents are the buzzy thing in the world of AI and concerns about threat vectors have been a theme this month. Disappointment in Apple's lack of intelligence from apple intelligence 12 months after making a big noise about it. Some interesting articles about AI and jobs , IBM & Anthropic disagreeing on the future for devs & AI and women in a conundrum as AI helps their careers as quickly as it's eliminating jobs. How do you test an AI when it's aware it's being tested for alignment? This is a conundrum that has some attention! The superintelligence strategy is fascinating to read and if you don't have time to read it  then you can  watch the video about it. Vibe coding was in the news. Simon Willison's post has a good explanation ( Don't do this for production code !) . The UKs ambitions to roll out AI is hitting the obvious  road blocks as you can't just say we need to embrace AI without investing to support that ambition with everything required from updating hardware, the software available to staff through to recruiting and training staff. The first step however is to acknowledge the scale of the ambition so that's a start. [MCP](https://modelcontextprotocol.io/introduction) seems to be gaining traction as the way to connect AI agents/ LLMs to external tools and systems. It was open sourced [by Anthropic](https://www.anthropic.com/news/model-context-protocol) in November last year 
*  [Life in Low Data Gravity | AI News & Insights](https://www.deeplearning.ai/the-batch/life-in-low-data-gravity/) 
* [[2503.12072] Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models](https://arxiv.org/abs/2503.12072)** **
* [Training Large Language Models for Advanced Typosquatting Detection](https://arxiv.org/abs/2503.22406v1) 
* [OpenAI's Sora Has a Small Problem With Being Hugely Racist and Sexist](https://futurism.com/openai-sora-racist-sexist) 
* [AI's "Biggest Test" Is Turning Into a Catastrophe as CoreWeave Flounders](https://futurism.com/ai-test-coreweave) 
* [Ghost Jobs, Deepfakes, and Bots: Welcome to the AI Job Hunt](https://www.eweek.com/news/ai-job-applicants-hiring/) 
* [ML and LLM Adoption Challenged Most Often by Observability - The New Stack](https://thenewstack.io/ml-and-llm-adoption-challenged-most-often-by-observability/) 
* [LLM providers on the cusp of an 'extinction' phase as capex realities bite](https://www.theregister.com/2025/03/31/llm_providers_extinction/)  
* [Calling all fashion models … now AI is coming for you](https://www.theguardian.com/fashion/2025/mar/30/fashion-models-ai-job-losses) 
* [Horseless intelligence | Ned Batchelder](https://nedbatchelder.com/blog/202503/horseless_intelligence.html)
* [The State of AI: Global survey | McKinsey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)
* [MCP is the new interface for security tools](https://mayakaczorowski.com/blogs/mcp) 
* [Bill Gates Predicts AI Will Replace Many Doctors, Teachers Within 10 Years](https://www.eweek.com/news/bill-gates-ai-predictions-jobs/) 
* [Why do LLMs make stuff up? New research peers under the hood. - Ars Technica](https://arstechnica.com/ai/2025/03/why-do-llms-make-stuff-up-new-research-peers-under-the-hood/) 
    * [Tracing the thoughts of a large language model \ Anthropic](https://www.anthropic.com/research/tracing-thoughts-language-model) 
* [[2502.08586] Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks](https://arxiv.org/abs/2502.08586)
* [Why Centralized AI Fails in Enterprise: The Case for a Federated Architecture - The New Stack](https://thenewstack.io/why-centralized-ai-fails-in-enterprise-the-case-for-a-federated-architecture/) 
* [Exploring Generative AI](https://martinfowler.com/articles/exploring-gen-ai.html#memo-13) 
* [Measuring AI Ability to Complete Long Tasks - METR](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) 
* [Study: AI Turns Evil After Training on Insecure Code - The New Stack](https://thenewstack.io/study-ai-turns-evil-after-training-on-insecure-code/) 
    * [Emergent Misalignment](https://www.emergent-misalignment.com/) 
* [A new, challenging AGI test stumps most AI models | TechCrunch](https://techcrunch.com/2025/03/24/a-new-challenging-agi-test-stumps-most-ai-models/) 
* [Is the AI Bubble About To Burst?](https://www.eweek.com/news/ai-bubble-risk/) 
* [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf) ( 2025 version)
* [Feeling Addicted to ChatGPT? You’re Not Alone, According to MIT & OpenAI](https://www.eweek.com/news/chatgpt-addiction-openai-mit-study/) 
* [Government AI roll-outs threatened by outdated IT systems | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/mar/26/government-ai-roll-outs-threatened-by-outdated-it-systems) 
* [MPs warn legacy IT could derail UK government's AI hopes • The Register](https://www.theregister.com/2025/03/26/legacy_systems_uk_ai/) 
* [How vibe coding will affect Engineering Managers ](https://newsletter.manager.dev/p/effect-of-ai-on-engineering-managers) 
* [Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation](https://arxiv.org/abs/2503.19622) 
* [Early methods for studying affective use and emotional well-being on ChatGPT | OpenAI](https://openai.com/index/affective-use-study) 
* [Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813) 
* [[2503.17489] Judge Anything: MLLM as a Judge Across Any Modality](https://arxiv.org/abs/2503.17489) 
* [[2503.14499] Measuring AI Ability to Complete Long Tasks](https://arxiv.org/abs/2503.14499) 
* [Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment](https://arxiv.org/abs/2411.18688v3) 
* [Blending AI and DevSecOps: Enhancing Security in the Development Pipeline](https://devops.com/blending-ai-and-devsecops-enhancing-security-in-the-development-pipeline) 
* [Joint studies from OpenAI and MIT found links between loneliness and ChatGPT use](https://www.engadget.com/ai/joint-studies-from-openai-and-mit-found-links-between-loneliness-and-chatgpt-use-193537421.html) 
* [Not all AI-assisted programming is vibe coding (but vibe coding rocks)](https://simonwillison.net/2025/Mar/19/vibe-coding/) 
* [Scientist Says That ChatGPT Has a "Staggering" Gender Problem](https://futurism.com/scientist-chatgpt-gender-gap) 
* [[2503.16416] Survey on Evaluation of LLM-based Agents](https://arxiv.org/abs/2503.16416) 
* [[2503.15299] Inside-Out: Hidden Factual Knowledge in LLMs](https://arxiv.org/abs/2503.15299) 
* [[2503.16031] Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031) 
* [[2503.13657] Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657) 
* [Commentary: Why China is suddenly flooding the market with powerful AI models - CNA](https://www.channelnewsasia.com/commentary/china-open-source-ai-model-deepseek-alibaba-compete-us-tech-5010401) 
* [VLMs as GeoGuessr Masters—Exceptional Performance, Hidden Biases, and Privacy Risks Mind the Photos You Post: AI Knows Where You Are!](https://arxiv.org/html/2502.11163v1) 
* [Superintelligence Strategy](https://www.nationalsecurity.ai/)
    * [Ex-Google CEO Says AI War Is COMING! (Superintelligence Strategy)](https://youtu.be/eMQulv3nVZk?si=HiDwhIfGleKwYa_S) 
* [North Korea launches new unit with a focus on AI hacking, per report | TechCrunch](https://techcrunch.com/2025/03/20/north-korea-launches-new-unit-with-a-focus-on-ai-hacking-per-report/) 
* [Vibe Coding is a Dangerous Fantasy | N’s Blog](https://nmn.gl/blog/vibe-coding-fantasy) 
* [OpenAI Scientists' Efforts to Make an AI Lie and Cheat Less Backfired Spectacularly](https://futurism.com/openai-stop-ai-lie-cheat-backfired) 
    * [[2503.11926] Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation](https://arxiv.org/abs/2503.11926) 
* [[2503.05710] AGI, Governments, and Free Societies](https://arxiv.org/abs/2503.05710) 
* [[2209.00626] The Alignment Problem from a Deep Learning Perspective](https://arxiv.org/abs/2209.00626) 
* [Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations — Apollo Research](https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations) 
* [Auditing language models for hidden objectives \ Anthropic](https://www.anthropic.com/research/auditing-hidden-objectives) 
* [RFK Says AI Nurses Are 'As Good as Any Doctor'](https://www.eweek.com/news/ai-healthcare-nurses-doctors/) 
* [[2501.11433] One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor](https://arxiv.org/abs/2501.11433) 
* [Now you don’t even need code to be a programmer. But you do still need expertise | John Naughton | The Guardian](https://www.theguardian.com/technology/2025/mar/16/ai-software-coding-programmer-expertise-jobs-threat) 
* [AI project failure rates are on the rise: report | CIO Dive](https://www.ciodive.com/news/AI-project-fail-data-SPGlobal/742590/) 
* [No one knows what the hell an AI agent is | TechCrunch](https://techcrunch.com/2025/03/14/no-one-knows-what-the-hell-an-ai-agent-is/) 
* [AI Agents in Doubt: Reducing Uncertainty in Agentic Workflows - The New Stack](https://thenewstack.io/ai-agents-in-doubt-reducing-uncertainty-in-agentic-workflows/) 
* [Majority of AI Researchers Say Tech Industry Is Pouring Billions Into a Dead End](https://futurism.com/ai-researchers-tech-industry-dead-end) 
* [OWASP Dishes Out Key Ingredients for a Secure Agentic AI Future | by Idan Habler | Intuit Engineering | Mar, 2025 | Medium](https://medium.com/intuit-engineering/owasp-dishes-out-key-ingredients-for-a-secure-agentic-ai-future-be862e167d6c) 
* [Trained on buggy code, LLMs often parrot same mistakes • The Register](https://www.theregister.com/2025/03/19/llms_buggy_code/)
    * [[2503.11082] LLMs are Bug Replicators: An Empirical Study on LLMs' Capability in Completing Bug-prone Code](https://arxiv.org/abs/2503.11082) 
* [[2503.12545] PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models](https://arxiv.org/abs/2503.12545) 
* [How To Create the Generative AI Policy You Needed Yesterday](https://thenewstack.io/how-to-create-the-generative-ai-policy-you-needed-yesterday/) 
* [Vibe Coding Is Here — But Are You Ready for Incident Vibing? - The New Stack](https://thenewstack.io/vibe-coding-is-here-but-are-you-ready-for-incident-vibing/) 
* [Billions of Dollars Down the Drain? 76% of AI Researchers Consider AGI 'Unrealistic Goal'](https://www.eweek.com/news/news-ai-investments-agi/) 
* [AI search is starting to kill Google’s ‘ten blue links’](https://www.theverge.com/ai-artificial-intelligence/631352/ai-search-adobe-analytics-google-perplexity-openai) 
* [Marvel Directors on AI in Films: 'Artists Need to Lead the Innovation'](https://www.eweek.com/news/marvel-directors-joe-anthony-russo-ai-films/) 
* [Post-apocalyptic education - by Ethan Mollick](https://www.oneusefulthing.org/p/post-apocalyptic-education) 
* [[2503.09669] Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.09669) 
* [Virginia Tech Study Reveals Machine Learning Models Struggle to Identify Critical Health Declines](https://bioengineer.org/virginia-tech-study-reveals-machine-learning-models-struggle-to-identify-critical-health-declines/) 
* [How AI Is Reshaping CISO Priorities - The New Stack](https://thenewstack.io/how-ai-is-reshaping-ciso-priorities/) 
* [AI is Creating New Paths for Women — And Closing Doors at the Same Time](https://www.eweek.com/news/news-ai-women-tech-jobs/) 
* [My Thoughts on the Future of "AI" Nicholas Carlini](https://nicholas.carlini.com/writing/2025/thoughts-on-future-ai.html) 
* [AI vs. Developer Jobs: Anthropic Predicts Replacement, IBM Pushes Back](https://www.eweek.com/news/ai-developers-jobs-anthropic-ibm/) 
* [Trust AI or Peers More for Advice? Here’s What Execs Prefer & Why](https://www.eweek.com/news/ai-trust-executives-sap-study/) 
* [Poisoning the Well and Other Generative AI Risks - The New Stack](https://thenewstack.io/poisoning-the-well-and-other-generative-ai-risks/) 
* [[2503.09837] On the Limitations of Vision-Language Models in Understanding Image Transforms](https://arxiv.org/abs/2503.09837) 
* [[2503.09905] Quantization for OpenAI's Whisper Models: A Comparative Analysis](https://arxiv.org/abs/2503.09905) 
* [[2503.10635] A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](https://arxiv.org/abs/2503.10635) 
* [DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more...](https://www.deeplearning.ai/the-batch/issue-292/) 
* [Daring Fireball: Something Is Rotten in the State of Cupertino](https://daringfireball.net/2025/03/something_is_rotten_in_the_state_of_cupertino) 
* [AI running out of juice despite Microsoft's hard squeezing • The Register](https://www.theregister.com/2025/03/14/ai_running_out_of_juice/) 
* [Entering AI Autumn: Why LLMs Are Nearing Their Limit - The New Stack](https://thenewstack.io/entering-ai-autumn-why-llms-are-nearing-their-limit/) 
* [Researchers astonished by tool’s apparent success at revealing AI’s “hidden objectives” - Ars Technica](https://arstechnica.com/ai/2025/03/researchers-astonished-by-tools-apparent-success-at-revealing-ais-hidden-motives/) 
* [How ProPublica Uses AI Responsibly in Its Investigations](https://www.propublica.org/article/using-ai-responsibly-for-reporting) 
* [‘A computer’s joke, on us’: writers respond to the short story written by AI | Books | The Guardian](https://www.theguardian.com/books/2025/mar/14/writers-respond-story-written-by-ai-sam-altman-chat-gpt-tracy-chevalier-kamila-shamsie-david-baddiel) 
* [The UK government embracing AI? I’m sorry, that’s nonsense and I can prove it | Chris Stokel-Walker | The Guardian](https://www.theguardian.com/commentisfree/2025/mar/14/uk-government-artifical-intelligence-chatgpt)
* [It turns out ChatGPT o1 and DeepSeek-R1 cheat at chess if they’re losing, which makes me wonder if I should trust AI with anything | TechRadar](https://www.techradar.com/computing/artificial-intelligence/it-turns-out-chatgpt-o1-and-deepseek-r1-cheat-at-chess-if-theyre-losing-which-makes-me-wonder-if-i-should-i-should-trust-ai-with-anything) 
* [Bank CEO says 4-day week isn't progressive, and AI will make it 'bloody logical' | Fortune Europe](https://fortune.com/europe/2025/03/13/atom-bank-ceo-running-4-day-work-week-cutting-working-hours-isnt-progressive-ai-make-bloody-logical/) 
* [[2503.05856] This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs](https://arxiv.org/abs/2503.05856) 
* [[2503.07389] TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.07389) 
* [[2503.07595] Detection Avoidance Techniques for Large Language Models](https://arxiv.org/abs/2503.07595) 
* [Pentesters: Is AI Coming for Your Role?](https://thehackernews.com/2025/03/pentesters-is-ai-coming-for-your-role.html) 
* [What Is LLM Observability and Monitoring? - The New Stack](https://thenewstack.io/what-is-llm-observability-and-monitoring/) 
* [Can You Trust AI To Be Your Data Analyst? - The New Stack](https://thenewstack.io/can-you-trust-ai-to-be-your-data-analyst/) 
* [[2503.03704] A Practical Memory Injection Attack against LLM Agents](https://arxiv.org/abs/2503.03704) 
* [[2502.09747] The Widespread Adoption of Large Language Model-Assisted Writing Across Society](https://arxiv.org/abs/2502.09747) 
* [What’s Missing With AI-Generated Code? Refactoring - The New Stack](https://thenewstack.io/whats-missing-with-ai-generated-code-refactoring/) 
* [The State of LLM Reasoning Models](https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling) ( Great article on scaling for inference)
* [Anthropic Warns White House: Act Now on AI Security or Face Serious Risks](https://www.eweek.com/news/anthropic-white-house-ai-security/) 
*  [[2503.02191] Understanding and Predicting Derailment in Toxic Conversations on GitHub](https://arxiv.org/abs/2503.02191) 
* [[2503.04369] Lost in Literalism: How Supervised Training Shapes Translationese in LLMs](https://arxiv.org/abs/2503.04369) 
* [Freelancers Are Getting Ruined by AI](https://futurism.com/freelancers-struggling-compete-ai) 
* [[2503.02879] Wikipedia in the Era of LLMs: Evolution and Risks](https://arxiv.org/abs/2503.02879) 
* [[2503.02846] Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs](https://arxiv.org/abs/2503.02846) 
* [GitHub - lmgame-org/GamingAgent: Computer gaming agents that run on your PC and laptops.](https://github.com/lmgame-org/GamingAgent) 
* [Why AI Agents Suck So Bad - The New Stack](https://thenewstack.io/why-ai-agents-suck-so-bad/) 
* [AGI still a long way off, academics in China have calculated • The Register](https://www.theregister.com/2025/03/05/boffins_from_china_calculate_agi/) 
    * [https://github.com/jingtaozhan/IntelligenceTest](https://github.com/jingtaozhan/IntelligenceTest) 
* [How New AI Agents Will Transform Credential Stuffing Attacks](https://thehackernews.com/2025/03/how-new-ai-agents-will-transform.html?m=1) 
* [Why AI Agents Suck So Bad - The New Stack](https://thenewstack.io/why-ai-agents-suck-so-bad/) 
* [AI-Powered Lawyering: AI Reasoning Models, Retrieval Augmented Generation, and the Future of Legal Practice](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5162111) 
* [Analysts Warn That If AI Agents Succeed, the "Internet Will Go Dark"](https://futurism.com/ai-agents-dark-internet) 
* [Power Cut](https://www.wheresyoured.at/power-cut/) 
* [AI is killing some companies, yet others are thriving - let's look at the data](https://www.elenaverna.com/p/ai-is-killing-some-companies-yet) 
* [[2502.15840] Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents](https://arxiv.org/abs/2502.15840) 
* [[2502.19187] BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187) 
* [If the best defence against AI is more AI, this could be tech’s Oppenheimer moment | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/mar/02/ai-oppenheimer-moment-karp-zapiska-technological-republic) 
* [[2502.16750] Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System](https://arxiv.org/abs/2502.16750) 
* [AI killing some companies, others doing fine](https://boingboing.net/2025/03/01/ai-killing-some-companies-others-doing-fine.html) 
* [Hallucinations in code are the least dangerous form of LLM mistakes](https://simonwillison.net/2025/Mar/2/hallucinations-in-code/) 
* [[2409.04109] Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://arxiv.org/abs/2409.04109) 
* [OpenAI May Have Really Screwed Up With GPT-4.5](https://futurism.com/openai-screwed-up-gpt-4-5) 
* [Will the future of software development run on vibes?](https://arstechnica.com/ai/2025/03/is-vibe-coding-with-ai-gnarly-or-reckless-maybe-some-of-both/) 
* [Sergey Brin says AGI is within reach if Googlers work 60-hour weeks - Ars Technica](https://arstechnica.com/google/2025/02/sergey-brin-says-agi-is-within-reach-if-googlers-work-60-hour-weeks/) 
* [Google’s Sergey Brin Says Engineers Should Work 60-Hour Weeks in Office to Build AI That Could Replace Them](https://gizmodo.com/googles-sergey-brin-says-engineers-should-work-60-hour-weeks-in-office-to-build-ai-that-could-replace-them-2000570025) 
* [[2502.09747] The Widespread Adoption of Large Language Model-Assisted Writing Across Society](https://arxiv.org/abs/2502.09747) 
* [Disrupting a global cybercrime network abusing generative AI - Microsoft On the Issues](https://blogs.microsoft.com/on-the-issues/2025/02/27/disrupting-cybercrime-abusing-gen-ai/) 
* [How AI Takeover Might Happen in 2 Years — LessWrong](https://www.lesswrong.com/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years) 
* [[2502.15737] A Performance Analysis of You Only Look Once Models for Deployment on Constrained Computational Edge Devices in Drone Applications](https://arxiv.org/abs/2502.15737)
* Feb had concerns about energy & water usage that the DCs would need . Muttering about the ROI from AI after fourth quarter earnings reports. Another theme was about AI making humans lazy when it came to critical thinking . Great paper on why AI  benchmarks should be taken with a huge spoonful of salt! The Register article summarises the paper in their usual snarky way.
* [[2502.01822] Firewalls to Secure Dynamic LLM Agentic Networks](https://arxiv.org/abs/2502.01822) 
* [AUTOHIJACKER: AUTOMATIC INDIRECT PROMPT INJECTION AGAINST BLACK-BOX LLM AGENTS](https://openreview.net/pdf?id=2VmB01D9Ef) 
* [[2502.13172] Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172)
* [AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways](https://dl.acm.org/doi/full/10.1145/3716628) 
* [OAuth Works for AI Agents, but Scaling Is Another Question - The New Stack](https://thenewstack.io/oauth-works-for-ai-agents-but-scaling-is-another-question/) 
    * [AI agent identity: it's just OAuth](https://mayakaczorowski.com/blogs/ai-agent-authentication) 
* [No new engineer hires this year as AI coding tools boost productivity, says Salesforce](https://www.theregister.com/2025/02/27/salesforce_misses_revenue_guidance/) 
* [U.S. Workers Are More Worried Than Hopeful About Future AI Use in the Workplace | Pew Research Center](https://www.pewresearch.org/social-trends/2025/02/25/u-s-workers-are-more-worried-than-hopeful-about-future-ai-use-in-the-workplace/) 
* [Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks](https://arxiv.org/abs/2502.13175v2) 
* [How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities](https://arxiv.org/abs/2502.18387v1) 
* [UK universities warned to ‘stress-test’ assessments as 92% of students use AI](https://www.theguardian.com/education/2025/feb/26/uk-universities-warned-to-stress-test-assessments-as-92-of-students-use-ai) 
* [[2502.17424] Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424) 
* [Investigating LLM Jailbreaking of Popular Generative AI Web Products](https://unit42.paloaltonetworks.com/jailbreaking-generative-ai-web-products/) 
* [How to exploit top LRMs that reveal their reasoning steps • The Register](https://www.theregister.com/2025/02/25/chain_of_thought_jailbreaking/)
    * [https://github.com/dukeceicenter/jailbreak-reasoning-openai-o1o3-deepseek-r1](https://github.com/dukeceicenter/jailbreak-reasoning-openai-o1o3-deepseek-r1)  
* [[2502.14975] Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries](https://arxiv.org/abs/2502.14975) 
* [Claude's extended thinking \ Anthropic](https://www.anthropic.com/research/visible-extended-thinking) ( now this is a benchmark I find worth using !) 
* [[2502.06329] Expect the Unexpected: FailSafe Long Context QA for Finance](https://arxiv.org/abs/2502.06329) 
* [Hypocrite Elon Musk Is Criticizing OpenAI for Not Open Sourcing ChatGPT While Refusing to Do the Same With Grok](https://futurism.com/hypocrite-elon-musk-criticizing-openai-open-sourcing-chatgpt-grok) 
* [Microsoft Backing Out of Expensive New Data Centers After Its CEO Expressed Doubt About AI Value](https://futurism.com/microsoft-ceo-hesitation-ai-expensive-data-centers) 
* [The Smarter AI Gets, the More It Start Cheating When It's Losing](https://futurism.com/the-byte/ai-cheating-chess) 
* [AI-Powered Deception is a Menace to Our Societies](https://thehackernews.com/2025/02/ai-powered-deception-is-menace-to-our.html?m=1)
* [The Death of Critical Thinking Will Kill Us Long Before AI.](https://www.joanwestenberg.com/the-death-of-critical-thinking-will-kill-us-long-before-ai/) 
* [[2502.12769] How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769)
* [[2502.11995] Presumed Cultural Identity: How Names Shape LLM Responses](https://arxiv.org/abs/2502.11995)  
* [[2502.13946] Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946) 
* [New Junior Developers Can’t Actually Code | N’s Blog](https://nmn.gl/blog/ai-and-learning) 
* [[2502.12464] SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464) 
* [How Hackers Manipulate Agentic AI with Prompt Engineering - SecurityWeek](https://www.securityweek.com/how-hackers-manipulate-agentic-ai-with-prompt-engineering/) 
* [AI can fix bugs—but can’t find them: OpenAI’s study highlights limits of LLMs in software engineering | VentureBeat](https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/) 
* ['Hopeless' to potentially handy: law firm puts AI to the test](https://www.bbc.com/news/articles/c743j83d8kzo) 
* [Should ‘Open Source AI’ Mean Exposing All Training Data?](https://shujisado.org/2025/02/18/should-open-source-ai-mean-exposing-all-training-data/) 
* [[2502.04376] MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf](https://www.arxiv.org/abs/2502.04376) 
* [ChatGPT Operator: Prompt Injection Exploits & Defenses · Embrace The Red](https://embracethered.com/blog/posts/2025/chatgpt-operator-prompt-injection-exploits/) 
* [Over 40% of AI-Related Data Breaches Tied to Cross-Border AI Use by 2027](https://www.eweek.com/news/improper-cross-border-ai-use-gartner/) 
* [The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers](https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf) 
* [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/pdf/2401.11817) 
* [Airbnb CEO says it's still too early for AI trip planning | TechCrunch](https://techcrunch.com/2025/02/14/airbnb-ceo-says-its-still-too-early-for-ai-trip-planning) 
* [The Emperor Has No Clothes: Security In The Age Of Deepfakes](https://www.linkedin.com/pulse/emperor-has-clothes-security-age-deepfakes-jim-liddle-bvxse) 
* [[2502.03628] The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering](https://arxiv.org/abs/2502.03628) 
* [If the AI Roundheads go to war with tech royalty, don’t bet against them | John Naughton | The Guardian](https://www.theguardian.com/technology/2025/feb/15/if-the-ai-roundheads-go-to-war-with-tech-royalty-dont-bet-against-them)
* [Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2502.06872) 
* [AI-Powered Social Engineering: Ancillary Tools and Techniques](https://thehackernews.com/2025/02/ai-powered-social-engineering-ancillary.html) 
* [European boffins want AI model tests put to the test • The Register](https://www.theregister.com/2025/02/15/boffins_question_ai_model_test/) 
    * [[2502.06559] Can We Trust AI Benchmarks? An Interdisciplinary Review of Current Issues in AI Evaluation](https://arxiv.org/abs/2502.06559) 
* [[2502.08680] Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical Ranges](https://arxiv.org/abs/2502.08680) 
* [I met the ‘godfathers of AI’ in Paris – here’s what they told me to really worry about | Alexander Hurst | The Guardian](https://www.theguardian.com/commentisfree/2025/feb/14/ai-godfathers-paris-industry-dangers-future) 
* [As Tech Companies Keep Pouring Money Into AI, Signs May Be Pointing to Disaster](https://futurism.com/tech-investment-ai) 
* [Securing AI Supply Chain: Like Software, Only Not - Google Cloud Community](https://www.googlecloudcommunity.com/gc/Community-Blog/Securing-AI-Supply-Chain-Like-Software-Only-Not/ba-p/867409) 
* [Larry Ellison wants to put all US data in one big AI system • The Register](https://www.theregister.com/2025/02/12/larry_ellison_wants_all_data/) 
* [After Copilot trial, government staff rated Microsoft's AI less useful than expected](https://www.theregister.com/2025/02/12/australian_treasury_copilot_pilot_assessment/) 
* [4 Takeaways from the Paris AI Summit: Global Tensions, China Alliances, and Billion-Dollar Investments](https://www.eweek.com/news/paris-ai-summit-roundup/) 
* [Is AI Helping or Hurting Critical Thought?](https://www.eweek.com/news/ai-critical-thinking-impact/) 
* [EU will put over $200 billion toward AI development](https://www.engadget.com/ai/eu-will-put-over-200-billion-toward-ai-development-150036706.html) 
* [Security Is Blocking AI Adoption: Is BYOC the Answer? - The New Stack](https://thenewstack.io/security-is-blocking-ai-adoption-is-byoc-the-answer/) 
* [Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch](https://arxiv.org/abs/2501.18512v1) 
* [AI impact hits mid-to-high wage occupations like IT the most • The Register](https://www.theregister.com/2025/02/11/ai_impact_hits_midtohigh_wage_jobs/?utm_source=dlvr.it&utm_medium=bluesky) 
* [Microsoft Study Finds AI Makes Human Cognition “Atrophied and Unprepared”](https://www.404media.co/microsoft-study-finds-ai-makes-human-cognition-atrophied-and-unprepared-3/) 
* [Where AI Benchmarks Fall Short, and How To Evaluate Models Instead - The New Stack](https://thenewstack.io/where-ai-benchmarks-fall-short-and-how-to-evaluate-models-instead/) 
* [OpenAI Seems to Be Low Key Panicking](https://futurism.com/openai-low-key-panicking-deepseek) 
* [Adversarial Misuse of Generative AI | Google Cloud Blog](https://cloud.google.com/blog/topics/threat-intelligence/adversarial-misuse-generative-ai) 
* [[2501.18636] SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model](https://arxiv.org/abs/2501.18636) 
* [[2501.14334] Exploring the sustainable scaling of AI dilemma: A projective study of corporations' AI environmental impacts](https://arxiv.org/abs/2501.14334)
*  [[2501.17749] Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation](https://arxiv.org/abs/2501.17749) 
* [OpenEuroLLM: Europe’s New Initiative for Open-Source AI Development - InfoQ](https://www.infoq.com/news/2025/02/open-euro-llm/) 
* [AI-Powered Social Engineering: Reinvented Threats](https://thehackernews.com/2025/02/ai-powered-social-engineering.html?m=1)
* [I tested ChatGPT’s deep research with the most misunderstood law on the internet](https://www.theverge.com/openai/607587/chatgpt-deep-research-hands-on-section-230) 
* [Elon Musk's DOGE Training an AI to Analyze Government Spending](https://futurism.com/elon-musk-doge-ai-government) 
* [Gradual Disempowerment](https://gradual-disempowerment.ai/) 
* [Call to make tech firms report data centre energy use as AI booms | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/feb/07/call-to-make-tech-firms-report-data-centre-energy-use-as-ai-booms) 
* [Concern the UK's AI ambitions could lead to water shortages](https://www.bbc.com/news/articles/ce85wx9jjndo) 
* [How Agentic AI will be Weaponized for Social Engineering Attacks - SecurityWeek](https://www.securityweek.com/how-agentic-ai-will-be-weaponized-for-social-engineering-attacks/) 
* [Tech Billionaire’s Solution to Fixing the Internet Broken by AI? Utopia For Content Creators](https://www.eweek.com/news/cloudflare-matthew-prince-internet-utopia-ai/) 
* [Researchers trained an OpenAI rival in half an hour for less than $50 | The Verge](https://www.theverge.com/news/607341/researchers-cheaper-openai-rival-training) 
* [IT decision makers unconvinced of returns from AI investment • The Register](https://www.theregister.com/2025/02/06/lenovo_ai_report/) 
* [Amazon's Mitigating AI Hallucinations Through This Mathematical Method](https://www.eweek.com/news/amazon-automated-reasoning-generative-ai-hallucinations/) 
* [Google ending AI arms ban incredibly concerning, campaigners say](https://www.bbc.com/news/articles/cy081nqx2zjo) 
* [DeepSeek has ripped away AI’s veil of mystique. That’s the real reason the tech bros fear it | Kenan Malik | The Guardian](https://www.theguardian.com/commentisfree/2025/feb/02/deepseek-ai-veil-of-mystique-tech-bros-fear) 
* [AI systems could be ‘caused to suffer’ if consciousness achieved, says research | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/feb/03/ai-systems-could-be-caused-to-suffer-if-consciousness-achieved-says-research) 
* [AI ‘godfather’ predicts another revolution in the tech in next five years | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/feb/04/ai-godfather-predicts-another-revolution-in-the-tech-in-next-five-years) 
* [OpenAI says its models are more persuasive than 82 percent of Reddit users - Ars Technica](https://arstechnica.com/ai/2025/02/are-ais-getting-dangerously-good-at-persuasion-openai-says-not-yet/) 
* [Prompt Injection for Large Language Models - InfoQ](https://www.infoq.com/articles/large-language-models-prompt-injection-stealing/) 
* [Constitutional Classifiers: Defending against universal jailbreaks \ Anthropic](https://www.anthropic.com/research/constitutional-classifiers) 
    * [[2501.18837] Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://arxiv.org/abs/2501.18837) 
* [How an AI-written book shows why the tech 'terrifies' creatives - BBC News](https://www.bbc.co.uk/news/articles/cp8k5gezykyo) 
* [OpenAI Staff Turn Guns on Each Other After DeepSeek Humiliation](https://futurism.com/openai-staff-turn-on-each-other-deepseek) 
* [DeepSeek, ChatGPT, Grok … which is the best AI assistant? We put them to the test | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/feb/01/deepseek-chatgpt-grok-gemini-claude-meta-ai-which-is-the-best-ai-assistant-we-put-them-to-the-test) 
* [Was this the week DeepSeek started the slow unwinding of the AI bet?](https://www.theguardian.com/technology/2025/feb/01/was-this-the-week-deepseek-started-the-slow-unwinding-of-the-ai-bet) 
* January was swamped by deepseek news but there was other news if you look hard enough 
* [How DeepSeek and next-generation AI agents could erode value of language models](https://www.cnbc.com/2025/01/31/deepseek-next-generation-ai-agents-may-erode-value-of-large-models.html) 
* [[2501.18009] Large Language Models Think Too Fast To Explore Effectively](https://arxiv.org/abs/2501.18009) 
* [[2501.18438] o3-mini vs DeepSeek-R1: Which One is Safer?](https://arxiv.org/abs/2501.18438) 
* [OpenAI Strikes Deal With US Government to Use Its AI for Nuclear Weapon Security](https://futurism.com/openai-signs-deal-us-government-nuclear-weapon-security) 
* [The Manhattan Project Was Secret. Should America’s AI Work Be Too? - WSJ](https://www.wsj.com/tech/ai/the-manhattan-project-was-secret-should-americas-ai-work-be-too-5638be21?st=iepZkH&reflink=desktopwebshare_permalink) 
* [UC Berkeley researchers claim to replicate DeepSeek AI for just $30](https://tribune.net.ph/amp/story/2025/01/31/uc-berkeley-researchers-claim-to-replicate-deepseek-ai-for-30) 
* [DeepSeek stole our tech .... says Open AI](https://youtu.be/hpwoGjpYygI?si=2PzadCwUm1yYWw_P)
* [Microsoft is forming a new unit to study AI's impacts | TechCrunch](https://techcrunch.com/2025/01/31/microsoft-is-forming-a-new-unit-to-study-ais-impacts/) 
* [Oh, I’m sorry, tech bros – did DeepSeek copy your work? I can hardly imagine your distress | Marina Hyde | The Guardian](https://www.theguardian.com/commentisfree/2025/jan/31/tech-bros-deepseek-china-sam-altman-openai) 
* [OpenAI Asking for Tens of Billions in New Investment to "Fund Its Money-Losing business Operations"](https://futurism.com/openai-asking-investment) 
* [The west is already losing the AI arms race | Larry Elliott | The Guardian](https://www.theguardian.com/commentisfree/2025/jan/30/ai-arms-race-china-deepseek) 
* [Another OpenAI Researcher Quits, Calls AI "Terrifying" and a "Risky Gamble" | eWEEK](https://www.eweek.com/news/open-ai-researcher-quits-calls-ai-terrifying/) 
* [The questions the Chinese government doesn’t want DeepSeek AI to answer - Ars Technica](https://arstechnica.com/ai/2025/01/the-questions-the-chinese-government-doesnt-want-deepseek-ai-to-answer/) 
* [Deep Impact](https://www.wheresyoured.at/deep-impact/) 
* [Dario Amodei — On DeepSeek and Export Controls](https://darioamodei.com/on-deepseek-and-export-controls) 
* [OpenAI ‘reviewing’ allegations that its AI models were used to make DeepSeek](https://www.theguardian.com/technology/2025/jan/29/openai-chatgpt-deepseek-china-us-ai-models)  
* [AI and the future of national security](https://blog.google/technology/safety-security/ai-and-the-future-of-national-security/) 
* [OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole From Us](https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/) 
* [DeepSeek advances could heighten safety risk, says ‘godfather’ of AI | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/jan/29/deepseek-artificial-intelligence-ai-safety-risk-yoshua-bengio)
* [What International AI Safety report says on jobs, climate, cyberwar and more | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/jan/29/what-international-ai-safety-report-says-jobs-climate-cyberwar-deepfakes-extinction) 
* [Former OpenAI safety researcher brands pace of AI development ‘terrifying’ | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/jan/28/former-openai-safety-researcher-brands-pace-of-ai-development-terrifying)
* [Is DeepSeek really sending data to China? Let’s decode | VentureBeat](https://venturebeat.com/ai/is-deepseek-really-sending-data-to-china-lets-decode/) 
* [DeepSeek R1 Exposed: Security Flaws in China’s AI Model • KELA Cyber Threat Intelligence](https://www.kelacyber.com/blog/deepseek-r1-security-flaws/) 
* [Zuckerberg Convening Huge "War Rooms" to Figure Out How a Chinese Startup Is Annihilating Meta's AI](https://futurism.com/zuckerberg-war-rooms-meta-ai-deepseek) 
* [The Short Case for Nvidia Stock | YouTube Transcript Optimizer](https://youtubetranscriptoptimizer.com/blog/05_the_short_case_for_nvda) 
* [AI Alignment in Practice: What It Means and How to Get It - The New Stack](https://thenewstack.io/ai-alignment-in-practice-what-it-means-and-how-to-get-it/) 
* [OpenAI Developer Seethes at Success of DeepSeek](https://futurism.com/openai-developer-seethes-deepseek) 
* [Experts urge caution over use of Chinese AI DeepSeek](https://www.theguardian.com/technology/2025/jan/28/experts-urge-caution-over-use-of-chinese-ai-deepseek) 
* [DeepSeek: Trump warns of 'wake-up call' for US tech firms - BBC News](https://www.bbc.co.uk/news/articles/c4gpq01rvd4o) 
* [DeepSeek FAQ – Stratechery by Ben Thompson](https://stratechery.com/2025/deepseek-faq/) 
* [The argument against AI agents and unnecessary automation • The Register](https://www.theregister.com/2025/01/27/ai_agents_automate_argument/) 
* [[2501.13011] MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011) 
* [Frontier AI systems have surpassed the self-replicating red line](https://arxiv.org/html/2412.12140v1#abstract)
* [We Aren't Being Told the Real Extent of AI Datacenter Emissions](https://futurism.com/the-byte/not-being-told-datacenter-emissions) 
* [Artificial Intelligence In Health And Health Care: Priorities For Action | Health Affairs](https://goo.gle/3CoOb2y) 
* [Trading inference-time compute for adversarial robustness | OpenAI](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/) 
* [There's Apparently a Huge Financial Problem With Trump's Massive AI Project](https://futurism.com/huge-financial-problem-trump-ai-stargate) 
* [[2501.12206] Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model](https://arxiv.org/abs/2501.12206) 
* [More AI, More Problems for Software Developers in 2025 - The New Stack](https://thenewstack.io/more-ai-more-problems-for-software-developers-in-2025/) 
* [Cloud Deployment of AI Models Jumps, Says Data Science Study - The New Stack](https://thenewstack.io/cloud-deployment-of-ai-models-jumps-says-data-science-study/) 
* [Can AI Pass Humanity’s Ultimate Intelligence Test?](https://www.eweek.com/news/can-ai-pass-ultimate-iq-test/) 
* [What happens when we can’t build bigger datacenters anymore? • The Register](https://www.theregister.com/2025/01/24/build_bigger_ai_datacenters/) 
* [Meta's Top AI Lawyer Quits in Disgust](https://futurism.com/the-byte/meta-ai-lawyer-quits) 
* [Trump's Staff Have Had It With Elon Musk's Insubordination](https://futurism.com/the-byte/trump-staff-furious-elon-musk) 
* [Why everyone in AI is freaking out about DeepSeek | VentureBeat](https://venturebeat.com/ai/why-everyone-in-ai-is-freaking-out-about-deepseek/) 
* [AI Hype Is Dropping Off a Cliff While Costs Soar, Experts Warn](https://futurism.com/the-byte/ai-hype-cliff-costs) 
* [Trump Revokes Biden EO Addressing AI Risks: What It Means](https://www.eweek.com/news/trump-revokes-biden-ai-eo/) 
* [Elon Musk and Sam Altman take to social media to fight over Stargate | TechCrunch](https://techcrunch.com/2025/01/22/elon-musk-and-sam-altman-take-to-social-media-to-fight-over-stargate/) 
* [Banks must keep ahead of risks and reap AI rewards • The Register](https://www.theregister.com/2025/01/21/banks_must_keep_ahead_of/) 
* [UK aims to fix government IT with help from AI Humphrey • The Register](https://www.theregister.com/2025/01/21/ai_humphrey_uk_government/) 
* [The Pentagon says AI is speeding up its 'kill chain' | TechCrunch](https://techcrunch.com/2025/01/19/the-pentagon-says-ai-is-speeding-up-its-kill-chain/)
* [What I've learned about writing AI apps so far | Seldo.com](https://seldo.com/posts/what-ive-learned-about-writing-ai-apps-so-far) 
* [Oscar-Nominated Film Editor’s AI Use Might Cost It the Academy Award](https://www.eweek.com/news/ai-use-might-cost-film-an-oscar/) 
* [AI Mistakes Are Very Different from Human Mistakes - Schneier on Security](https://www.schneier.com/blog/archives/2025/01/ai-mistakes-are-very-different-from-human-mistakes.html) 
* [Share of teens using ChatGPT for schoolwork doubled from 2023 to 2024 | Pew Research Center](https://www.pewresearch.org/short-reads/2025/01/15/about-a-quarter-of-us-teens-have-used-chatgpt-for-schoolwork-double-the-share-in-2023/) 
* [AI isn’t very good at history, new paper finds | TechCrunch](https://techcrunch.com/2025/01/19/ai-isnt-very-good-at-history-new-paper-finds/) 
* [More AI, More Problems for Software Developers in 2025 - The New Stack](https://thenewstack.io/more-ai-more-problems-for-software-developers-in-2025/) 
* [AI benchmarking organization criticized for waiting to disclose funding from OpenAI | TechCrunch](https://techcrunch.com/2025/01/19/ai-benchmarking-organization-criticized-for-waiting-to-disclose-funding-from-openai/)
* [Cringing before the tech giants is no way to make Britain an AI superpower | John Naughton | The Guardian](https://www.theguardian.com/commentisfree/2025/jan/19/cringing-before-the-tech-giants-is-no-way-to-make-britain-an-ai-superpower) 
* [Labour’s investment in AI isn’t as clever as it thinks it is | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/jan/17/labour-investment-in-ai-isnt-as-clever-as-it-thinks-it-is) 
* [Character.AI Is Seeking Partnerships with Media Brands, Despite Still Facing Two Lawsuits over the Welfare of Minor Users](https://futurism.com/character-ai-seeking-partnerships-despite-lawsuits) 
* [Amazon Says All It Needs to Do Before Releasing an AI-Powered Alexa Is to Solve the Giant Engineering Problem That Nobody Else on Earth Has Been Able to Solve](https://futurism.com/amazon-ai-powered-alexa-hallucinations-problem) 
* [Before Apple's AI Went Haywire and Started Making Up Fake News, Its Engineers Warned of Deep Flaws With the Tech](https://futurism.com/the-byte/apple-engineers-ai-deep-flaws) 
* [Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography](https://arxiv.org/pdf/2501.08970) 
* [[2501.08365] Towards Best Practices for Open Datasets for LLM Training](https://arxiv.org/abs/2501.08365) 
* [CEO of AI Music Company Says People Don’t Like Making Music](https://www.404media.co/ceo-of-ai-music-company-says-people-dont-like-making-music/) ( it's obvious why he says this after all it's his business model)
* [[2501.05542] Infecting Generative AI With Viruses](https://arxiv.org/abs/2501.05542) 
* [Amazon races to transplant Alexa’s ‘brain’ with generative AI](https://arstechnica.com/ai/2025/01/amazon-must-solve-hallucination-problem-before-launching-ai-enabled-alexa/) 
* [Lessons from red teaming 100 generative AI products](https://airedteamwhitepapers.blob.core.windows.net/lessonswhitepaper/MS_AIRT_Lessons_eBook.pdf) 
    * [avrix paper -Lessons From Red Teaming 100 Generative AI Products](https://arxiv.org/pdf/2501.07238) 
* [AI Agents Are Here. What Now?](https://huggingface.co/blog/ethics-soc-7) 
* [PM outlines AI Opportunities Action Plan | TechMarketView](https://www.techmarketview.com/ukhotviews/archive/2025/01/13/pm-outlines-ai-opportunities-action-plan)
* [Artificial Intelligence: Plan to 'unleash AI' across UK revealed - BBC News](https://www.bbc.co.uk/news/articles/crr05jykzkxo) ( Not entirely sure when any of this would start realising a ROI )  
* [Generative AI – The Power and the Glory](https://simonwillison.net/2025/Jan/12/generative-ai-the-power-and-the-glory/) 
* [While AI Causes Climate Change, Nvidia Scientist Proposes AI-Powered Firefighting Robots](https://futurism.com/ai-climate-change-wildfires) 
* [Artificial intelligence: 41% of companies worldwide plan to reduce workforces by 2030 due to AI | CNN Business](https://edition.cnn.com/2025/01/08/business/ai-job-losses-by-2030-intl/index.html) 
* [Survey: AI Tools are Increasing Amount of Bad Code Needing to be Fixed - DevOps.com](https://devops.com/survey-ai-tools-are-increasing-amount-of-bad-code-needing-to-be-fixed/) 
* [Code Quality Becomes Even More Vital in the AI Era - The New Stack](https://thenewstack.io/code-quality-becomes-even-more-vital-in-the-ai-era/) 
* [Elon Musk says all human data for AI training ‘exhausted’ | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/2025/jan/09/elon-musk-data-ai-training-artificial-intelligence) 
* [CEO Says He Hasn’t Hired Anyone in a Year as He Replaces Human Workers With AI](https://futurism.com/the-byte/klarna-ceo-ai-replacing-workers) 
* [Trolley Problem, Safety Versus Security of Generative AI - SecurityWeek](https://www.securityweek.com/trolley-problem-safety-versus-security-of-generative-ai/) 
* [AI-supported spear phishing fools more than 50% of targets | Malwarebytes](https://www.malwarebytes.com/blog/news/2025/01/ai-supported-spear-phishing-fools-more-than-50-of-targets) 
* [Not even OpenAI's $200/mo ChatGPT Pro plan can turn a profit • The Register](https://www.theregister.com/2025/01/06/altman_gpt_profits/) 
* [AI Domination: Remote Controlling ChatGPT ZombAI Instances · Embrace The Red](https://embracethered.com/blog/posts/2025/spaiware-and-chatgpt-command-and-control-via-prompt-injection-zombai/) 
* [‘Virtual employees’ could join workforce as soon as this year, OpenAI boss says | Technology sector | The Guardian](https://www.theguardian.com/business/2025/jan/06/virtual-employees-could-join-workforce-as-soon-as-this-year-openai-boss-says) 
* [Meta's AI Profiles Are Indistinguishable From Terrible Spam That Took Over Facebook](https://www.404media.co/metas-ai-profiles-are-indistinguishable-from-terrible-spam-that-took-over-facebook/)
* [Apple opts everyone into having their Photos analyzed by AI • The Register](https://www.theregister.com/2025/01/03/apple_enhanced_visual_search/) 
* [Why AI Did Not Upend the Super Year of Elections | Lawfare](https://www.lawfaremedia.org/article/why-ai-did-not-upend-the-super-year-of-elections) 
* [People Are Disgusted by Facebook’s Plan to Deploy AI-Powered “Users”](https://futurism.com/disgusted-facebook-ai-users)
