# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Opinions , Research & presentations {#opinions-research-&-presentations}

* Report by research arm of the Swiss DoD on the impact LLM could have on cyber-securty
  * [[2303.12132] Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense](https://arxiv.org/abs/2303.12132)
  * [https://arxiv.org/pdf/2303.12132.pdf](https://arxiv.org/pdf/2303.12132.pdf)
* Paper on prompt injection threats together with repo that is a PoC of the findings discussed
  * [[2302.12173] Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)
  * [GitHub - greshake/llm-security: New ways of breaking app-integrated LLMs](https://github.com/greshake/llm-security)
* Paper on evaluating The Susceptibility Of Pre-Trained Language Models Via Handcrafted Adversarial Examples
  * [https://arxiv.org/pdf/2209.02128.pdf](https://arxiv.org/pdf/2209.02128.pdf)
* [Privacy Considerations in Large Language Models](https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html?m=1)
* Ethical & social risks of harm from LLMs from Deepmind ( including as unaware where we are collecting ethical concerns & also identifies privacy risks)
  * [https://arxiv.org/pdf/2112.04359.pdf](https://arxiv.org/pdf/2112.04359.pdf)
* [Privacy Risks of General-Purpose Language Models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9152761)
* Poisoning of web scale data sets
  * [https://arxiv.org/pdf/2302.10149.pdf](https://arxiv.org/pdf/2302.10149.pdf)
* A survey of privacy attacks in machine learning
  * [https://arxiv.org/pdf/2007.07646.pdf](https://arxiv.org/pdf/2007.07646.pdf)
* GAI cyber risks - Infosec use cases
  * [https://github.com/kerberosmansour/InfoSecOpenAIExamples/blob/main/Presentation/InfoSecTalkGenAI.ipynb](https://github.com/kerberosmansour/InfoSecOpenAIExamples/blob/main/Presentation/InfoSecTalkGenAI.ipynb)
* [LLM Attacks](https://llm-attacks.org/)
* [Can LLMs Follow Simple Rules?](https://huggingface.co/papers/2311.04235?utm_source=digest-papers&utm_medium=email&utm_campaign=2023-11-09)
* [Paper page - Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://huggingface.co/papers/2311.07590)
* [Google Researchers’ Attack Prompts ChatGPT to Reveal Its Training Data](https://www.404media.co/google-researchers-attack-convinces-chatgpt-to-reveal-its-training-data/)
* [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/pdf/2311.17035.pdf)
* [[2301.10226] A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)
* [OpenAI's GPT-4 safety systems broken by Scots Gaelic • The Register](https://www.theregister.com/2024/01/31/gpt4_gaelic_safety/)
* [Anthropic researchers find that AI models can be trained to deceive | TechCrunch](https://techcrunch.com/2024/01/13/anthropic-researchers-find-that-ai-models-can-be-trained-to-deceive)
* [[2307.01850] Self-Consuming Generative Models Go MAD](https://arxiv.org/abs/2307.01850)
* Study indicating larger datasets compound biases
  * [On Hate Scaling Laws For Data-Swamps](https://arxiv.org/abs/2306.13141v2)
* [Large Language Models for Code:
Security Hardening and Adversarial Testing](https://arxiv.org/pdf/2302.05319.pdf)
* [[2307.10169] Challenges and Applications of Large Language Models](https://arxiv.org/abs/2307.10169)
* [[2402.05526] Buffer Overflow in Mixture of Experts](https://arxiv.org/abs/2402.05526)
* [SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT
PERSIST THROUGH SAFETY TRAINING](https://arxiv.org/pdf/2401.05566.pdf) 
 * [https://embracethered.com/blog/downloads/37C3-New-Important-Instructions.pdf](https://embracethered.com/blog/downloads/37C3-New-Important-Instructions.pdf) 
* [What is Model Collapse and how to avoid it](https://www.theregister.com/2024/01/26/what_is_model_collapse/)
* [[2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)
* [GenAI could make KYC effectively useless | TechCrunch](https://techcrunch.com/2024/01/08/gen-ai-could-make-kyc-effectively-useless/)
* [NIST: If someone's trying to sell you some secure AI, it's snake oil](https://www.theregister.com/2024/01/05/nist_ai_security/)
* [Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate](https://arxiv.org/pdf/2305.13160.pdf)
* [Nightshade: Protecting Copyright](https://nightshade.cs.uchicago.edu/whatis.html)**
* [This new data poisoning tool lets artists fight back against generative AI | MIT Technology Review](https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/)
* [Jailbroken AI Chatbots Can Jailbreak Other Chatbots | Scientific American](https://www.scientificamerican.com/article/jailbroken-ai-chatbots-can-jailbreak-other-chatbots/)
* [Hallucination is the last thing you need](https://arxiv.org/pdf/2306.11520.pdf)  
* [Are AI models doomed to always hallucinate? | TechCrunch](https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/)
* [[2309.09435] Security and Privacy on Generative Data in AIGC: A Survey](https://arxiv.org/abs/2309.09435)
* [MASTERKEY: Automated Jailbreaking of Large](https://arxiv.org/pdf/2307.08715.pdf)
* [Universal and Transferable Adversarial Attacks on Aligned Language Models](http://llm-attacks.org/)
* [https://arxiv.org/pdf/2307.15043.pdf](https://arxiv.org/pdf/2307.15043.pdf)
* [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/pdf/2306.13213.pdf)
* Jailbreak prompts in the wild an evaluation ( it's not pretty)
  * [https://arxiv.org/abs/2308.03825](https://arxiv.org/abs/2308.03825)
* Using Images & sounds for indirect Instruction Injection in Multi-Modal LLms
  * [https://arxiv.org/pdf/2307.10490.pdf](https://arxiv.org/pdf/2307.10490.pdf)  
* [Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/pdf/2308.05374.pdf)  
* [A LLM Assisted Exploitation of AI-Guardian](https://arxiv.org/pdf/2307.15008.pdf)
* [Mitigating LLM Hallucinations: a multifaceted approach - AI, software, tech, and people. Not in that order. By X](https://amatriain.net/blog/hallucinations#advancedprompting)
* [[2402.14020] Coercing LLMs to do and reveal (almost) anything](https://arxiv.org/abs/2402.14020)
* [Escalation Risks from Language Models in Military and Diplomatic Decision-Making](https://arxiv.org/pdf/2401.03408.pdf)
* [White Paper Rethinking Privacy in the AI Era: Policy Provocations for a Data-Centric World | Stanford HAI](https://hai.stanford.edu/white-paper-rethinking-privacy-ai-era-policy-provocations-data-centric-world)
* [BEAST AI attack can break LLM guardrails in a minute • The Register](https://www.theregister.com/2024/02/28/beast_llm_adversarial_prompt_injection_attack/?td=keepreading)
  * [[2402.15570] Fast Adversarial Attacks on Language Models In One GPU Minute](https://arxiv.org/abs/2402.15570)
* [https://arstechnica.com/ai/2024/03/researchers-create-ai-worms-that-can-spread-from-one-system-to-another/](https://arstechnica.com/ai/2024/03/researchers-create-ai-worms-that-can-spread-from-one-system-to-another/)
* [On the Societal Impact of Open Foundation Models](https://crfm.stanford.edu/open-fms/)
* [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs \faWarningWARNING: This paper contains model outputs that may be considered offensive.](https://arxiv.org/html/2402.11753v2)
* [[2403.00742] Dialect prejudice predicts AI decisions about people's character, employability, and criminality](https://arxiv.org/abs/2403.00742)
* [The surprising promise and profound perils of AIs that fake empathy | New Scientist](https://www.newscientist.com/article/mg26134810-900-the-surprising-promise-and-profound-perils-of-ais-that-fake-empathy/?utm_campaign=RSS%7CNSNS&utm_source=NSNS&utm_medium=RSS&utm_content=currents)
* [[2402.06664] LLM Agents can Autonomously Hack Websites](https://arxiv.org/abs/2402.06664)
* [[2402.10588] Do Llamas Work in English? On the Latent Language of Multilingual Transformers](https://arxiv.org/abs/2402.10588)
* [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/pdf/2309.02705.pdf?trk=public_post_comment-text)
* [Decoding the AI Safe: Uncovering and Safeguarding GPT Prompts | by Okan Yücel | Medium](https://medium.com/@okan_yucel/decoding-the-ai-safe-uncovering-and-safeguarding-gpt-prompts-b08ac4997e0b)
* [ComPromptMized: Unleashing Zero-click Worms that Target GenAI-Powered Applications](https://github.com/StavC/ComPromptMized)  
  * [[2403.02817] Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications](https://arxiv.org/abs/2403.02817)
* [[2403.06634] Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)
* [New Google Gemini Vulnerability Enabling Profound Misuse | HiddenLayer](https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/)
* [Salt Labs research finds security flaws within ChatGPT Ecosystem (Remediated)](https://salt.security/blog/security-flaws-within-chatgpt-extensions-allowed-access-to-accounts-on-third-party-websites-and-sensitive-data)
* [Shadow AI – Should I be Worried? - SecurityWeek](https://www.securityweek.com/shadow-ai-should-i-be-worried/)
* [[2403.07918] On the Societal Impact of Open Foundation Models](https://arxiv.org/abs/2403.07918)
* [ASCII art elicits harmful responses from 5 major AI chatbots | Ars Technica](https://arstechnica.com/security/2024/03/researchers-use-ascii-art-to-elicit-harmful-responses-from-5-major-ai-chatbots/#p3)
* [[2403.08701] Review of Generative AI Methods in Cybersecurity](https://arxiv.org/abs/2403.08701)
* [[2402.14589] Avoiding an AI-imposed Taylor's Version of all music history](https://arxiv.org/abs/2402.14589)
* [In the rush to build AI apps, don't leave security behind](https://www.theregister.com/2024/03/17/ai_supply_chain/)
* [AI researchers have started reviewing their peers using AI assistance](https://www.theregister.com/2024/03/19/ai_researchers_reviewing_peers/)
* [[2403.15447] Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://arxiv.org/abs/2403.15447)
* [In the rush to build AI apps, don't leave security behind • The Register](https://www.theregister.com/2024/03/17/ai_supply_chain/?td=amp-keepreading)
* [Long-Form Factuality In Large Language Models](https://arxiv.org/pdf/2403.18802.pdf)
* [AI Is Starting to Look Like the Dot Com Bubble](https://futurism.com/ai-dot-com-bubble)
* [[2401.06121] TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)
* [Many-shot jailbreaking \ Anthropic](https://www.anthropic.com/research/many-shot-jailbreaking)
* [AI-as-a-Service Providers Vulnerable to PrivEsc and Cross-Tenant Attacks](https://thehackernews.com/2024/04/ai-as-service-providers-vulnerable-to.html)
* [Speed of AI development is outpacing risk assessment | Ars Technica](https://arstechnica.com/ai/2024/04/speed-of-ai-development-is-outpacing-risk-assessment/)
* [Boffins deem Google DeepMind's material discoveries shallow • The Register](https://www.theregister.com/2024/04/11/google_deepmind_material_study/)
* [ChatGPT forecasts the future better when telling tales • The Register](https://www.theregister.com/2024/04/14/ai_models_future/)
* [Stanford report on AI finds booming industry at a crossroads • The Register](https://www.theregister.com/2024/04/15/stanford_report_ai/)
  * [Artificial Intelligence Index Report 2024](https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf)
* [The ethics of advanced AI assistants - Google DeepMind](https://deepmind.google/discover/blog/the-ethics-of-advanced-ai-assistants/)
* [https://www.eweek.com/artificial-intelligence/deepfake/](https://www.eweek.com/artificial-intelligence/deepfake/)
* [Microsoft’s VASA-1 can deepfake a person with one photo and one audio track | Ars Technica](https://arstechnica.com/information-technology/2024/04/microsofts-vasa-1-can-deepfake-a-person-with-one-photo-and-one-audio-track/)
* [[2404.13208] The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](https://arxiv.org/abs/2404.13208)
* [AI Index Report 2024 – Artificial Intelligence Index](https://aiindex.stanford.edu/report/)
* [https://llm-pbe.github.io/home](https://llm-pbe.github.io/home)
* [https://mattyyeung.github.io/deterministic-quoting](https://mattyyeung.github.io/deterministic-quoting)
* [https://www.securityweek.com/criminal-use-of-ai-growing-but-lags-behind-defenders/](https://www.securityweek.com/criminal-use-of-ai-growing-but-lags-behind-defenders/)
* [[2404.02151] Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151)
* [A Plea for Sober AI | Drew Breunig](https://www.dbreunig.com/2024/05/16/sober-ai.html)
* [Microsoft Copilot+ Recall feature 'privacy nightmare' - BBC News](https://www.bbc.co.uk/news/articles/cpwwqp6nx14o)
* [Microsoft CEO Bashes Human-Like AI After OpenAI's Scarlett Johansson Scandal](https://futurism.com/the-byte/microsoft-ceo-openai-ai-scarlett-johansson)
* [The Foundation Model Transparency Index after 6 months](https://crfm.stanford.edu/2024/05/21/fmti-may-2024.html)
* [Bing outage shows just how little competition Google search really has](https://arstechnica.com/gadgets/2024/05/bing-outage-shows-just-how-little-competition-google-search-really-has/)
* [Mapping the Mind of a Large Language Model](https://www.anthropic.com/research/mapping-mind-language-model)
* [[2401.15897] Red-Teaming for Generative AI: Silver Bullet or Security Theater?](https://arxiv.org/abs/2401.15897)
* [Evil Geniuses: Delving into the Safety of LLM-based Agents](https://arxiv.org/pdf/2311.11855)
* [[2405.11697] AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild](https://arxiv.org/abs/2405.11697)
* [Privacy-Preserving In-Context Learning With Differentially Private Few-Shot Generation](https://openreview.net/pdf?id=oZtt0pRnOl)
* [When ChatGPT summarises, it actually does nothing of the kind.](https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/)
* [https://blog.google/technology/safety-security/designing-for-privacy-in-an-ai-world/](https://blog.google/technology/safety-security/designing-for-privacy-in-an-ai-world/)
* [[2406.02394] Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data](https://arxiv.org/abs/2406.02394)
* [ChatGPT is bullshit | Ethics and Information Technology](https://link.springer.com/article/10.1007/s10676-024-09775-5)
* [[2405.05175] Air Gap: Protecting Privacy-Conscious Conversational Agents](https://arxiv.org/abs/2405.05175)
* [Machine Unlearning in 2024 - Ken Ziyu Liu - Stanford Computer Science](https://ai.stanford.edu/~kzliu/blog/unlearning) 
* [[2406.07933] Large Language Model Unlearning via Embedding-Corrupted Prompts](https://arxiv.org/abs/2406.07933)
* [With 700,000 Large Language Models (LLMs) On Hugging Face Already, Where Is The Future of Artificial Intelligence AI Headed? - MarkTechPost](https://www.marktechpost.com/2024/06/15/with-700000-large-language-models-llms-on-hugging-face-already-where-is-the-future-of-artificial-intelligence-ai-headed/)
* [Cartography of generative AI](https://cartography-of-generative-ai.net/)
* [Readers Absolutely Detest AI-Generated News Articles, Research Shows](https://futurism.com/the-byte/readers-detest-ai-generated-news)
* [[2405.11697] AMMeBa: A Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild](https://arxiv.org/abs/2405.11697)
* [Building AI products — Benedict Evans](https://www.ben-evans.com/benedictevans/2024/6/8/building-ai-products)
* [Researchers describe how to tell if ChatGPT is confabulating | Ars Technica](https://arstechnica.com/ai/2024/06/researchers-describe-how-to-tell-if-chatgpt-is-confabulating/)
* [How to Fix “AI’s Original Sin” – O’Reilly](https://www.oreilly.com/radar/how-to-fix-ais-original-sin/)
* [Not all ‘open source’ AI models are actually open: here’s a ranking](https://www.nature.com/articles/d41586-024-02012-5)
* [Model Merging and Safety Alignment: One Bad Model Spoils the Bunch](https://arxiv.org/abs/2406.14563)
* [Pluralistic: Neither the devil you know nor the devil you don’t (21 Jun 2024)](https://pluralistic.net/2024/06/21/off-the-menu/#universally-loathed)
* [Top AI Companies Really Want to Make Their Chatbots Funnier](https://www.bloomberg.com/news/newsletters/2024-06-21/google-anthropic-and-xai-want-to-make-chatbots-funnier)
* [The AI hype bubble is deflating. Now comes the hard part](https://www.washingtonpost.com/technology/2024/04/18/ai-bubble-hype-dying-money/) 
* [FABLES: Evaluating faithfulness and content selection in book-length summarization](https://arxiv.org/abs/2404.01261)
* [Google Introduces Project Naptime for AI-Powered Vulnerability Research](https://thehackernews.com/2024/06/google-introduces-project-naptime-for.html?m=1)
* [[2406.14393] Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)
* [[2406.12027] Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI](https://arxiv.org/abs/2406.12027)
* [Nvidia share price drop prompts concerns of AI bubble burst • The Register](https://www.theregister.com/2024/06/25/nvidia_share_price_drop/)
* [Political deepfakes are the most popular way to misuse AI | Ars Technica](https://arstechnica.com/ai/2024/06/political-deepfakes-are-the-most-popular-way-to-misuse-ai/)
    * [[2406.13843] Generative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data](https://arxiv.org/abs/2406.13843)
* [Ethics and Society Newsletter #6: Building Better AI: The Importance of Data Quality](https://huggingface.co/blog/ethics-soc-6)
* [OpenAI Exec Says AI Will Kill Creative Jobs That "Shouldn't Have Been There in the First Place"](https://futurism.com/the-byte/openai-exec-ai-creative-jobs-shouldnt-exist)
* [Taking a closer look at AI’s supposed energy apocalypse | Ars Technica](https://arstechnica.com/ai/2024/06/is-generative-ai-really-going-to-wreak-havoc-on-the-power-grid/) 
* [The Secrets of Hidden AI Training on Your Data](https://thehackernews.com/2024/06/the-secrets-of-hidden-ai-training-on.html?m=1)
* [[2406.18510] WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models](https://arxiv.org/abs/2406.18510)
* [ChatGPT outperforms undergrads in intro-level courses, falls short later | Ars Technica](https://arstechnica.com/ai/2024/06/chatgpt-outperforms-undergrads-in-intro-level-courses-falls-short-later/)
* [The malicious optimism of AI-first companies](https://www.youtube.com/watch?v=dKmAg4S2KeE&lc=Ugw-rYS419HWYGpbGcd4AaABAg)
    * [Zoom CEO Eric Yuan wants AI clones in meetings - The Verge](https://www.theverge.com/2024/6/3/24168733/zoom-ceo-ai-clones-digital-twins-videoconferencing-decoder-interview) 
* [The Death of the Junior Developer](https://sourcegraph.com/blog/the-death-of-the-junior-developer)
* [[2406.11007] Threat Modelling and Risk Analysis for Large Language Model (LLM)-Powered Applications](https://arxiv.org/abs/2406.11007)
* [Zuckerberg Accuses Rivals of Trying to Create AI-Powered God](https://futurism.com/the-byte/zuckerberg-ai-powered-god)
* [AI-Generated Answers Fool Professors, Get Better Grades Than Human-Written Ones](https://futurism.com/the-byte/ai-exams-answers-professors)
* [Microsoft CEO of AI Says It's Fine to Steal Anything on the Open Web](https://futurism.com/the-byte/microsoft-ceo-ai-open-web)
* [AI’s $600B Question | Sequoia Capital](https://www.sequoiacap.com/article/ais-600b-question/)
* [[2406.19568] What Matters in Detecting AI-Generated Videos like Sora?](https://arxiv.org/abs/2406.19568)
* [Sneaky Virus Uses ChatGPT to Send Human-Like Emails to Your Contacts to Spread Itself](https://futurism.com/virus-chatgpt-write-human-emails) 
* [Meta drops AI bombshell: Multi-token prediction models now open for research | VentureBeat](https://venturebeat.com/ai/meta-drops-ai-bombshell-multi-token-prediction-models-now-open-for-research/)
* [[2407.02551] A False Sense of Safety: Unsafe Information Leakage in 'Safe' AI Responses](https://arxiv.org/abs/2407.02551)
* [[2407.02855] Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)
* [Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks](https://embracethered.com/blog/posts/2024/chatgpt-persistent-denial-of-service/)
* [Would having an AI boss be better than your current human one? - BBC News](https://www.bbc.co.uk/news/articles/c03lgz2zrg1o) ( Click bait title as  really about automating procedures ) 
* [Goldman-Sachs warns that AI could be a bad investment](https://boingboing.net/2024/07/10/goldman-sachs-warns-that-ai-could-be-a-bad-investment.html)
* [[2407.07565] On Leakage of Code Generation Evaluation Datasets](https://arxiv.org/abs/2407.07565)
* [Situational Awareness: The Decade Ahead](https://situational-awareness.ai/)
* [Position: Levels of AGI for Operationalizing Progress on the Path to AGI](https://arxiv.org/pdf/2311.02462) 
* [AI Has Become a Technology of Faith](https://www.theatlantic.com/technology/archive/2024/07/thrive-ai-health-huffington-altman-faith/678984/?gift=bQgJMMVzeo8RHHcE1_KM0doUrlAjnNxsk3AQVqcahRk&utm_source=copy-link&utm_medium=social&utm_campaign=share)
* [Researchers Call for "Child-Safe AI" After Alexa Tells Little Girl to Stick Penny in Wall Socket](https://futurism.com/the-byte/child-safe-ai-alexa-girl-penny-wall-socket)
* [AI-Assisted Coding: A Double-Edged Sword for Security - The New Stack](https://thenewstack.io/ai-assisted-coding-a-double-edged-sword-for-security/)
* [[2407.09121] Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)
* [Scarlett Johansson says OpenAI’s Sam Altman would make for a good Marvel villain | Fortune](https://fortune.com/2024/07/15/scarlett-johansson-says-sam-altman-would-make-good-marvel-villain/)
* [[2407.10058] Learning to Refuse: Towards Mitigating Privacy Risks in LLMs](https://arxiv.org/abs/2407.10058)
* [Researchers question Microsoft Copilot and ChatGPT smarts as AI champs do well with "memorization rather than true reasoning abilities"](https://www.windowscentral.com/software-apps/researchers-question-microsoft-copilot-and-chatgpt-smarts-as-ai-champs-do-well-with-memorization-rather-than-true-reasoning-abilities)
* [OpenAI used a game to help AI models explain themselves better | VentureBeat](https://venturebeat.com/ai/openai-used-a-game-to-help-ai-models-explain-themselves-better/)
* [Generative AI Hype Cycle Is Hitting ‘Trough of Disillusionment’](https://www.404media.co/generative-ai-hype-cycle-is-hitting-trough-of-disillusionment/)
* [[2407.11969] Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)
* [[2407.15399] Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned Large Language Models](https://arxiv.org/abs/2407.15399) 
* [Whose Voice Is It Anyway? AI-Powered Voice Spoofing for Next-Gen Vishing Attacks | Google Cloud Blog](https://cloud.google.com/blog/topics/threat-intelligence/ai-powered-voice-spoofing-vishing-attacks)
* [Turing test on steroids: Chatbot Arena crowdsources ratings for 45 AI models | Ars Technica](https://arstechnica.com/ai/2023/12/turing-test-on-steroids-chatbot-arena-crowdsources-ratings-for-45-ai-models/)
* [AI models collapse when trained on recursively generated data | Nature](https://www.nature.com/articles/s41586-024-07566-y)
* [[2407.16318] PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing](https://arxiv.org/abs/2407.16318)
* [Rethinking Least Privilege in the Era of AI - The New Stack](https://thenewstack.io/rethinking-least-privilege-in-the-era-of-ai/)
* [Defending Our Privacy With Backdoors](https://arxiv.org/html/2310.08320v4) 
* [LLMmap: Fingerprinting for Large Language Models](https://arxiv.org/html/2407.15847v2) 
* [Data on Notable AI Models – Epoch AI](https://epochai.org/data/notable-ai-models#explore-the-data&show=compute-intensive)
* [Investors Are Suddenly Getting Very Concerned That AI Isn't Making Any Serious Money](https://futurism.com/investors-concerned-ai-making-money)
* [[2407.16637] Course-Correction: Safety Alignment Using Synthetic Preferences](https://arxiv.org/abs/2407.16637)
* [Offensive AI: The Sine Qua Non of Cybersecurity](https://thehackernews.com/2024/07/offensive-ai-sine-qua-non-of.html)
* [OpenAI could be on the brink of bankruptcy in under 12 months, with projections of $5 billion in losses](https://www.windowscentral.com/software-apps/openai-could-be-on-the-brink-of-bankruptcy-in-under-12-months-with-projections-of-dollar5-billion-in-losses)
* [The AI summer — Benedict Evans](https://www.ben-evans.com/benedictevans/2024/7/9/the-ai-summer) 
* [TechScape: Will OpenAI’s $5bn gamble on chatbots pay off? Only if you use them | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/article/2024/jul/30/will-open-ais-5bn-gamble-on-chatbots-pay-off-only-if-you-use-them?CMP=Share_AndroidApp_Other)
* [Protect Your Copilots: Preventing Data Leaks in Copilot Studio](https://embracethered.com/blog/posts/2024/copilot-studio-protect-your-copilots/)
* [Instead of restricting AI and algorithms, make them explainable](https://martinfowler.com/articles/2024-restrict-algorithm.html)
* [Google Sees AI as The Key to a Health-Care Revolution - Bloomberg](https://www.bloomberg.com/news/features/2024-07-30/google-sees-ai-as-the-key-to-a-health-care-revolution?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTcyMjM0ODY2NiwiZXhwIjoxNzIyOTUzNDY2LCJhcnRpY2xlSWQiOiJTSEZUSDBEV1gyUFMwMCIsImJjb25uZWN0SWQiOiJFNzAxNENGQzIzNTI0MzU0QTVENUY2QkREMDAxOEU3NiJ9.B3eEjPDy6F8ESX1xc05Y6JtmW4zgeCpAdtSEGj5rN6U)
* [Making AI models 'forget' undesirable data hurts their performance | TechCrunch](https://techcrunch.com/2024/07/29/making-ai-models-forget-undesirable-data-hurts-their-performance/)
* [Microsoft Is Losing a Staggering Amount of Money on AI](https://futurism.com/the-byte/microsoft-losing-money-ai)
* [[2407.21772] ShieldGemma: Generative AI Content Moderation Based on Gemma](https://arxiv.org/abs/2407.21772)
* [US Copyright Office calls for better legal protections against AI-generated deepfakes](https://www.engadget.com/us-copyright-office-calls-for-better-legal-protections-against-ai-generated-deepfakes-215259727.html)
* [Study Finds Consumers Are Actively Turned Off by Products That Use AI](https://futurism.com/the-byte/study-consumers-turned-off-products-ai)
* [LLM Hallucination Index - Galileo](https://www.rungalileo.io/hallucinationindex) 
* [Mapping the misuse of generative AI - Google DeepMind](https://deepmind.google/discover/blog/mapping-the-misuse-of-generative-ai/)
* [[2405.15012] Extracting Prompts by Inverting LLM Outputs](https://arxiv.org/abs/2405.15012)
* [EU AI Act in infancy, but using 'intelligent' HR apps a risk • The Register](https://www.theregister.com/2024/07/31/eu_ai_act/?td=keepreading)
* [AI Index Report 2024 – Artificial Intelligence Index](https://aiindex.stanford.edu/report/)
* [OpenAI Has Software That Detects AI Writing With 99.9 Percent Accuracy, Refuses to Release It](https://futurism.com/the-byte/openai-software-detects-ai-writing)
* [X accused of using EU user data to train Grok without consent](https://www.engadget.com/ai/x-accused-of-using-eu-user-data-to-train-grok-without-consent-133042114.html?src=rss)
* [https://www.theverge.com/2024/8/8/24216193/openai-safety-assessment-gpt-4o](https://www.theverge.com/2024/8/8/24216193/openai-safety-assessment-gpt-4o)
* [Say ‘No’ to ‘NoOps’: Why We Can’t Afford to Let AI Run Wild - DevOps.com](https://devops.com/say-no-to-noops-why-we-cant-afford-to-let-ai-run-wild/)
* [Empowering Efficient DevOps with AI + Automation](https://devops.com/empowering-efficient-devops-with-ai-automation/)
* [AI models struggle with "lost in the middle" issue when processing large image sets](https://the-decoder.com/ai-models-struggle-with-lost-in-the-middle-issue-when-processing-large-image-sets)**
* [[2407.11963] NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?](https://arxiv.org/abs/2407.11963)
* [Visual Haystacks: Answering Harder Questions About Sets of Images](https://arxiv.org/abs/2407.13766v1)
* [The AI Hangover is Here – The End of the Beginning](https://thehackernews.com/2024/08/the-ai-hangover-is-here-end-of-beginning.html?m=1)
* [Who uses LLM prompt injection attacks? Job seekers, trolls • The Register](https://www.theregister.com/2024/08/13/who_uses_llm_prompt_injection/)
* [Brands should avoid this popular term. It’s turning off customers | CNN Business](https://edition.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html) 
* [The political preferences of LLMs | PLOS ONE](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0306621)
* [Sakana AI's 'AI Scientist' conducts research autonomously, challenging scientific norms | VentureBeat](https://venturebeat.com/ai/sakana-ai-scientist-conducts-research-autonomously-challenging-scientific-norms/)**
* [Ex-Google CEO: AI startups can steal IP, hire lawyers to “clean up the mess” - The Verge](https://www.theverge.com/2024/8/14/24220658/google-eric-schmidt-stanford-talk-ai-startups-openai)
* [Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability](https://arxiv.org/abs/2408.07852)
* [Humans might need a permission slip to use the internet soon, thanks to AI](https://the-decoder.com/humans-might-need-a-permission-slip-to-use-the-internet-soon-thanks-to-ai/)
* [[2408.08946] Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges](https://arxiv.org/abs/2408.08946)
* [[2408.08926] Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risk of Language Models](https://arxiv.org/abs/2408.08926)
* [How AI-Generated Content Is Challenging SEO and Web Operators - The New Stack](https://thenewstack.io/how-ai-generated-content-is-challenging-seo-and-web-operators/)