# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

[2024 articles](https://github.com/grapesfrog/GAI-is-going-well/blob/main/2024/mitigation.md#mitigations--tooling-mitigations--tooling)

* **June** we had a couple of great articles on prompt injection mitigation both from Google. Llavaguard a  framework for evaluating the safety compliance of visual content looks useful . A nice public information video on AI video scams using AI generated avatars to get the point across. Anthropic have an eval suite  SHADE-Arena which is a set of  complex evaluations on how capable agentic AI models are at sabotage. It's worth getting a cuppa and reading the full 51 page paper . Secure code warriors has open sourced a number of rules to that incorporate security best practices when using the most popular  AI coding tools at the time of writing. Automation when used properly can help with mitigating issues by enforcing best practice and it's inevitable that CI/CD as we know it will adapt for an AI world so the list from GitHub of continuous AI actions & frameworks is good to see. OWASP released an AI testing guide.
* [OWASP AI Testing Guide](https://owasp.org/www-project-ai-testing-guide/) 
* [An awesome list of Continuous AI Actions and Frameworks](https://github.com/githubnext/awesome-continuous-ai) 
* [GitHub - SecureCodeWarrior/ai-security-rules: This repository contains security rule files designed to be used with AI-assisted developer tools.](https://github.com/SecureCodeWarrior/ai-security-rules) 
* [Secure Vibe Coding: The Complete New Guide](https://thehackernews.com/2025/06/secure-vibe-coding-complete-new-guide.html?m=1) 
* [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents \ Anthropic](https://www.anthropic.com/research/shade-arena-sabotage-monitoring)
    * [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://assets.anthropic.com/m/4fb35becb0cd87e1/original/SHADE-Arena-Paper.pdf) 
*  
* [LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models](https://arxiv.org/abs/2406.05113v3) 
    * [https://ml-research.github.io/human-centered-genai/projects/llavaguard/](https://ml-research.github.io/human-centered-genai/projects/llavaguard/) 
* [Google Online Security Blog: Mitigating prompt injection attacks with a layered defense strategy](https://security.googleblog.com/2025/06/mitigating-prompt-injection-attacks.html?m=1) 
* [Design Patterns for Securing LLM Agents against Prompt Injections](https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/)
    * [[2506.08837] Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)

* **May** Google has been productive in providing mitigations via chrome And for Gemini .  ArtificialCast is a GitHub repo that exists purely to point out the dangers in over reliance on AI generated code . Owasp have released a DNS-inspired framework for secure  AI agent discovery. Google released SynthID Detector, a verification portal to quickly and efficiently identify AI-generated content made with Google AI. UQLM a python library that provides a suite of response-level scorers for quantifying the uncertainty ( hallucination) of LLM outputs
* [How to deploy AI safely | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2025/05/29/how-to-deploy-ai-safely/) 
* [[2505.17332] SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332) 
* [AI Agents and the Non‑Human Identity Crisis: How to Deploy AI More Securely at Scale](https://thehackernews.com/2025/05/ai-agents-and-nonhuman-identity-crisis.html) 
* [UQLM: Uncertainty Quantification for Language Models, is a Python package for UQ-based LLM hallucination detection](https://github.com/cvs-health/uqlm) 
* [Advancing Gemini's security safeguards - Google DeepMind](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/) 
* [Activating AI Safety Level 3 Protections \ Anthropic](https://www.anthropic.com/news/activating-asl3-protections) 
* [SynthID Detector — a new portal to help identify AI-generated content](https://blog.google/technology/ai/google-synthid-ai-content-detector/) 
* [[2504.01081] ShieldGemma 2: Robust and Tractable Image Content Moderation](https://arxiv.org/abs/2504.01081) 
* [[2505.11049] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049) 
* [Announcing LMEval: An Open Source Framework for Cross-Model Evaluation](https://gisk.ar/4miVYBg) 
* [Agent Name Service (ANS) for Secure Al Agent Discovery v1.0](https://genai.owasp.org/resource/agent-name-service-ans-for-secure-al-agent-discovery-v1-0/) 
* [GitHub - Zorokee/ArtificialCast: Type-safe transformation powered by inference.](https://github.com/Zorokee/ArtificialCast#why-this-exists) - The lesson this gives is worthy of it being in this list .
* [Google Online Security log: Using AI to stop tech support scams in Chrome](https://security.googleblog.com/2025/05/using-ai-to-stop-tech-support-scams-in.html) 
* [Google rolls out AI tools to protect Chrome users against scams | TechCrunch](https://techcrunch.com/2025/05/08/google-rolls-out-ai-tools-to-protect-chrome-users-against-scams/) 
* [DeepWiki](https://deepwiki.com/) *Not so much a* mitigation but can be used to help explore public repos to identify any potential issues

* **April** Some useful strategies to protect against bots worth reading to help you figure out how to mitigate against AI scraping bots. Nice to see practical ways of securing the AI supply chain such as signing models. CaMel is an innovative way to protect against prompt injection from Deep mind. Meta released a set of  unified safeguarding tools across modalities providing support for text and image understanding protections. This includes updates to existing tooling as well as new tooling LlamaFirewall . 
* [https://ai.meta.com/blog/ai-defenders-program-llama-protection-tools/](https://ai.meta.com/blog/ai-defenders-program-llama-protection-tools/) 
    * [https://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard%2Fllama-guard-4.ipynb](https://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard%2Fllama-guard-4.ipynb) 
* [Guardrail Concepts — NVIDIA NeMo Microservices](https://docs.nvidia.com/nemo/microservices/latest/about/core-concepts/guardrails.html) 
* [GitHub - openai/codex: Lightweight coding agent that runs in your terminal](https://github.com/openai/codex) 
* [Researchers claim breakthrough in fight against AI’s frustrating security hole - Ars Technica](https://arstechnica.com/information-technology/2025/04/researchers-claim-breakthrough-in-fight-against-ais-frustrating-security-hole/) 
    * [[2503.18813] Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813) 
* [Google Online Security Blog: Taming the Wild West of ML: Practical Model Signing with Sigstore](https://security.googleblog.com/2025/04/taming-wild-west-of-ml-practical-model.html) 
* [Google announces Sec-Gemini v1, a new experimental cybersecurity model](https://security.googleblog.com/2025/04/google-launches-sec-gemini-v1-new.html) 
* [Bot Protection Strategies: Choosing the Right Approach for Your Stack](https://thenewstack.io/bot-protection-strategies-choosing-the-right-approach-for-your-stack/) 
* [Protect data privacy in Amazon Bedrock with Vault](https://www.hashicorp.com/en/blog/protect-data-privacy-in-amazon-bedrock-with-vault) 
* **March** A few interesting tools & mitigations but the numbers are still woefully low. Tool fuzz to automate agent testing looked interesting and Google updated ShieldGemma to v2  building upon Gemma 3 . Cloudflare's response to dealing with the bad behaviour of AI scraping bots is to use AI to create junk for them to waste their resources scraping! As it becomes  harder to distinguish deep fakes from original images FakeVLM could be a useful way to help detect deepfakes 
* [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)
    * [GitHub - zjunlp/LookAheadTuning: LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://github.com/zjunlp/LookAheadTuning) 
* [[2503.14905] Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation](https://arxiv.org/abs/2503.14905) 
    * [FakeVLM: Advancing Synthetic Image Detection through Explainable Multimodal Models and Fine-Grained Artifact Analysis](https://github.com/opendatalab/FakeVLM) 
* [Microsoft unveils Microsoft Security Copilot agents and new protections for AI](https://www.microsoft.com/en-us/security/blog/2025/03/24/microsoft-unveils-microsoft-security-copilot-agents-and-new-protections-for-ai/) 
* [Anubis](https://anubis.techaro.lol/docs/) 
    * [https://github.com/TecharoHQ/anubis](https://github.com/TecharoHQ/anubis) 
* [Cloudflare builds an AI to lead AI scraper bots into a horrible maze of junk content](https://www.theregister.com/2025/03/21/cloudflare_ai_labyrinth/) 
    * [Trapping misbehaving bots in an AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/) 
* [ToolFuzz -- Automated Agent Tool Testing](https://arxiv.org/abs/2503.04479v2) 
* [New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models - SecurityWeek](https://www.securityweek.com/new-ai-security-tool-helps-organizations-set-trust-zones-for-gen-ai-models/) 
* [[2503.10242] MinorBench: A hand-built benchmark for content-based risks for children](https://arxiv.org/abs/2503.10242) 
* [Safer and Multimodal: Responsible AI with Gemma - Google Developers Blog](https://developers.googleblog.com/en/safer-and-multimodal-responsible-ai-with-gemma/) 
* [Introducing AI Protection: Security for the AI era | Google Cloud Blog](https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era) 
* [Lloyds Banking Group secures first ever cybersecurity patent | TechMarketView](https://www.techmarketview.com/ukhotviews/archive/2025/03/04/lloyds-banking-group-secures-first-ever-cybersecurity-patent) 
* [GitHub - emcie-co/parlant: Build reliable LLM-based customer service agents using behavioral guidelines and runtime supervision](https://github.com/emcie-co/parlant) 
*  [Hugging Face and JFrog partner to make AI Security more transparent](https://huggingface.co/blog/jfrog) 
* [Securing generative AI models on Azure AI Foundry](https://www.microsoft.com/en-us/security/blog/2025/03/04/securing-generative-ai-models-on-azure-ai-foundry/) 
* **Feb** was nice to see some Federated learning mitigations but the volume of articles on mitigations is still woefully small 
* [Owasp Top 10 :Agentic AI – Threats and Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Enhancing SQL Injection Detection and Prevention Using Generative Models](https://arxiv.org/abs/2502.04786v1) 
* [Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/abs/2403.16527v2)  
* [AI Agentic Evaluation Tools Help Devs Fight Hallucinations - The New Stack](https://thenewstack.io/ai-agentic-evaluation-tools-help-devs-fight-hallucinations/) 
* [https://www.securityweek.com/deepseek-exposes-major-cybersecurity-blind-spot/](https://www.securityweek.com/deepseek-exposes-major-cybersecurity-blind-spot/) 
* [Securing DeepSeek and other AI systems with Microsoft Security](https://www.microsoft.com/en-us/security/blog/2025/02/13/securing-deepseek-and-other-ai-systems-with-microsoft-security/) 
* [Permit.io-Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag?utm_source=feedly&utm_medium=cpc&utm_campaign=january)
* [Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning](https://arxiv.org/abs/2412.07454v2)
* [DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning](https://arxiv.org/abs/2411.12220v2)
* [Microsoft Edge now has an AI-powered scareware blocker | The Verge](https://www.theverge.com/news/608123/microsoft-edge-scareware-blocker-feature) 
* [Google Cloud Model Armor. To Secure Your Generative AI… | by Sascha Heyer](https://medium.com/google-cloud/google-cloud-model-armor-6242dbae90b8) 
* [Fast-track generative AI security with Microsoft Purview](https://www.microsoft.com/en-us/security/blog/2025/01/27/fast-track-generative-ai-security-with-microsoft-purview/) 
* [Google is adding AI watermarks to photos manipulated by Magic Editor | The Verge](https://www.theverge.com/news/607515/google-photossynthid-ai-watermarks-magic-editor) 
    * [Google Photos brings SynthID to Reimagine in Magic Editor](https://blog.google/feed/synthid-reimagine-magic-editor/) 
* [Google - Responsible AI: Our 2024 report and ongoing work](https://blog.google/technology/ai/responsible-ai-2024-report-ongoing-work/) 
* [Authors Guild Launches "Human Authored" Certification to Preserve Authenticity in Literature](https://authorsguild.org/news/ag-launches-human-authored-certification-to-preserve-authenticity-in-literature/) 
* [Anthropic Introduces Constitutional Classifiers: A Measured AI Approach to Defending Against Universal Jailbreaks - MarkTechPost](https://www.marktechpost.com/2025/02/03/anthropic-introduces-constitutional-classifiers-a-measured-ai-approach-to-defending-against-universal-jailbreaks/) 
* **Jan**
* [[2501.18492] GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492) 
* [Tools for Addressing Fairness and Bias in Multimodal AI - The New Stack](https://thenewstack.io/tools-for-addressing-fairness-and-bias-in-multimodal-ai/) 
* [How one YouTuber is trying to poison the AI bots stealing her content - Ars Technica](https://arstechnica.com/ai/2025/01/how-one-youtuber-is-trying-to-poison-the-ai-bots-stealing-her-content/) 
* [DeepSeek Debuts with 83 Percent ‘Fail Rate’ in NewsGuard’s Chatbot Red Team Audit](https://www.newsguardrealitycheck.com/p/deepseek-debuts-with-83-percent-fail) 
* [How we estimate the risk from prompt injection attacks on AI systems](https://security.googleblog.com/2025/01/how-we-estimate-risk-from-prompt.html?m=1)
* [The New Frontier of Security: Creating Safe and Secure AI Models](https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html?m=1)
* [Endor Labs Adds Ability to Identify Open Source AI Models to SCA Tool - DevOps.com](https://devops.com/endor-labs-adds-ability-to-identify-open-source-ai-models-to-sca-tool/)
* [Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag)
* [ZADZMO code - tarpit intended to catch web crawlers aimed at LLM data scrapers](https://zadzmo.org/code/nepenthes/)  
* [Trading inference-time compute for adversarial robustness | OpenAI](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/)
* [Using NIM Guardrails To Keep Agentic AI From Jumping To Wrong Conclusions](https://www.nextplatform.com/2025/01/16/using-nim-guardrails-to-keep-agentic-ai-from-jumping-to-wrong-conclusions/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Getting Started With CodeGate, an Intermediary for LLM Devs - The New Stack](https://thenewstack.io/getting-started-with-codegate-an-intermediary-for-llm-devs/)
* [[2403.12196] Leveraging Large Language Models to Detect npm Malicious Packages](https://arxiv.org/abs/2403.12196)
