# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

* [O2 unveils Daisy, the AI granny wasting scammers’ time](https://news.virginmediao2.co.uk/o2-unveils-daisy-the-ai-granny-wasting-scammers-time/) 
* [LLM Guard](https://llm-guard.com/)
* [Promptfoo](https://www.promptfoo.dev/) 
* [attacks & mitigations labs](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/responsible-ai)
* [From Naptime to Big Sleep: Using Large Language Models To Catch Vulnerabilities In Real-World Code](https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html?m=1) 
* [Google Photos will soon show you if an image was edited with AI - The Verge](https://www.theverge.com/2024/10/24/24278663/google-photos-generative-ai-label-reimagine-best-take)
* [Startup can identify deepfake video in real time - Ars Technica](https://arstechnica.com/security/2024/10/startup-can-catch-identify-deepfake-video-in-realtime/) 
* [Checks by Google: AI-Powered Compliance for Apps and Code - The New Stack](https://thenewstack.io/checks-by-google-ai-powered-compliance-for-apps-and-code/) 
* [[2410.02828] PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System](https://arxiv.org/abs/2410.02828) 
* [Fiddler Trust Service for LLM Scoring](https://www.fiddler.ai/tour/trust-service-for-llm-scoring) 
* [Microsoft claims its AI safety tool not only finds errors but also fixes them - The Verge](https://www.theverge.com/2024/9/24/24253452/microsoft-correction-ai-safety-tool-fix-errors)
* [Google will begin flagging AI-generated images in Search later this year | TechCrunch](https://techcrunch.com/2024/09/17/google-will-begin-flagging-ai-generated-images-in-search-later-this-year/)
* [Microsoft gives deepfake porn victims a tool to scrub images from Bing search | TechCrunch](https://techcrunch.com/2024/09/05/microsoft-gives-deepfake-porn-victims-a-tool-to-scrub-images-from-bing-search/)
* [DataGemma: Using real-world data to address AI hallucinations](https://blog.google/technology/ai/google-datagemma-ai-llm/)
* [Nonprofit scrubs illegal content from controversial AI training dataset | Ars Technica](https://arstechnica.com/tech-policy/2024/08/nonprofit-scrubs-illegal-content-from-controversial-ai-training-dataset/)
* [AI risk repository](https://airisk.mit.edu/)
* [[2408.04948] HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction](https://arxiv.org/abs/2408.04948)
* [NIST releases new tool to check AI models’ security](https://www-infoworld-com.cdn.ampproject.org/c/s/www.infoworld.com/article/3478308/nist-releases-new-tool-to-check-ai-models-security.html/amp/)
* [[2408.04284] LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection](https://arxiv.org/abs/2408.04284)
* [Enhancing LLM quality and interpretability with the Vertex AI Gen AI Evaluation Service](https://cloud.google.com/blog/products/ai-machine-learning/enhancing-llm-quality-and-interpretability-with-the-vertex-gen-ai-evaluation-service?e=48754805)
* [Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma - Google Developers Blog](https://dpmd.ai/4d0MKEH)
* [Gemma Scope: helping the safety community shed light on the inner workings of language models - Google DeepMind](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)
* [StopNCII.org](https://www.stopncii.org/)
* [Project Naptime: Evaluating Offensive Security Capabilities of Large Language Models](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html)
* [Search the YouTube Videos Secretly Powering Generative AI](https://www.proofnews.org/youtube-ai-search/)
* [Promptfoo](https://www.promptfoo.dev)
* [Astronomers discover technique to spot AI fakes using galaxy-measurement tools | Ars Technica](https://arstechnica.com/information-technology/2024/07/astronomers-discover-technique-to-spot-ai-fakes-using-galaxy-measurement-tools/)
* [OpenAI’s latest model will block the ‘ignore all previous instructions’ loophole - The Verge](https://www.theverge.com/2024/7/19/24201414/openai-chatgpt-gpt-4o-prompt-injection-instruction-hierarchy) ([ evaded within days apparently according to a post on  X](https://x.com/elder_plinius/status/1814023961535295918?s=46&t=dYaCeu-tFWb5QJSJ3axkiQ) )
* [Solving the data security challenge for AI builders](https://www.hashicorp.com/blog/solving-the-data-security-challenge-for-ai-builders)
* [[2407.07071] Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071)
* [Real criminals, fake victims: how chatbots are being deployed in the global fight against phone scammers | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/article/2024/jul/07/ai-chatbots-phone-scams)
* [Declare your AIndependence: block AI bots, scrapers and crawlers with a single click](https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click)
* [YouTube now lets you request removal of AI-generated content that simulates your face or voice | TechCrunch](https://techcrunch.com/2024/07/01/youtube-now-lets-you-request-removal-of-ai-generated-content-that-simulates-your-face-or-voice/)
* [[2406.18495] WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)
* [Mitigating Skeleton Key, a new type of generative AI jailbreak technique | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/06/26/mitigating-skeleton-key-a-new-type-of-generative-ai-jailbreak-technique/)
* [GitHub - lamini-ai/Lamini-Memory-Tuning: Banishing LLM Hallucinations Requires Rethinking Generalization](https://github.com/lamini-ai/Lamini-Memory-Tuning)
* [[2406.06443] LLM Dataset Inference: Did you train on my dataset?](https://arxiv.org/abs/2406.06443)
* [New Attack Technique 'Sleepy Pickle' Targets Machine Learning Models](https://thehackernews.com/2024/06/new-attack-technique-sleepy-pickle.html?m=1)
* [Balancing Innovation and Security | The Centre for GenAIOps](https://genaiops.ai/balancing-innovation-and-security)
* [Poisoning Data to Protect It – Communications of the ACM](https://cacm.acm.org/news/poisoning-data-to-protect-it/)
* [LLMs will always be dangerous. Security can only be delivered at the level of the application | The Centre for GenAIOps](https://genaiops.ai/llms-will-always-be-dangerous-security-can-only-be-delivered-at-the-level-of-the-application)
* [GenAIOps](https://genaiops.ai/) A global community built to pioneer, document and evangelise about the concept of GenAIOps.
* [Euro banks worry AI will increase their dependence on US big tech](https://www.theregister.com/2024/06/10/euro_banks_worry_ai_us_tech/)
* [Watermarking AI-generated text and video with SynthID - Google DeepMind](https://deepmind.google/discover/blog/watermarking-ai-generated-text-and-video-with-synthid/)
* [AI jailbreaks: What they are and how they can be mitigated | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/)
* [Awesome products for securing AI systems includes open source and commercial options and an infographic licensed CC-BY-SA-4.0.](https://github.com/zmre/awesome-security-for-ai)
* [https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/)
* [IbisPaint launches an AI Disturbance tool to make it harder for machines to copy your work](https://www.engadget.com/ibispaint-launches-an-ai-disturbance-tool-to-make-it-harder-for-machines-to-copy-your-work-131015685.html?src=rss)
* [TikTok becomes first platform to require watermarking of AI content](https://www.theregister.com/2024/05/10/tiktok_ai_watermarks/)
* [Understanding the source of what we see and hear online | OpenAI](https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online)
* [https://github.com/uw-nsl/SafeDecoding](https://github.com/uw-nsl/SafeDecoding)
* [Data Scientists Targeted by Malicious Hugging Face ML Models with Silent Backdoor](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor)
* [GitHub - protectai/rebuff: LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
* [https://www.infoq.com/news/2024/03/cloudflare-firewall-for-ai/](https://www.infoq.com/news/2024/03/cloudflare-firewall-for-ai/)
* [[2402.11755] SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755)
* [AI mishaps are surging – and now they're being tracked like software bugs](https://www.theregister.com/2024/03/08/ai_mishaps_are_surging_and/)
* [Introducing the Red-Teaming Resistance Leaderboard](https://huggingface.co/blog/leaderboard-haizelab)
* [https://www.eweek.com/artificial-intelligence/ai-governance-tools/](https://www.eweek.com/artificial-intelligence/ai-governance-tools/)
* [[2402.16822] Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822)
* [AI Watermarking 101: Tools and Techniques](https://huggingface.co/blog/watermarking)
* [Introducing Pebblo — Data Visibility & Governance for Gen-AI apps | by Sridhar Ramaswamy | Feb, 2024 | Medium](https://medium.com/@sridhar_ramaswamy/introducing-pebblo-data-visibility-governance-for-gen-ai-apps-086ca8a62d10)
* [GitHub - Azure/PyRIT: The Python Risk Identification Tool for generative AI (PyRIT) is an open access automation framework to empower security professionals and machine learning engineers to proactively find risks in their generative AI systems.](https://github.com/Azure/PyRIT)
* [Responsible Generative AI Toolkit | Gemma | Google AI for Developers](https://ai.google.dev/responsible)
* [Unveiling Our Latest Innovation - ​​Pindrop® Pulse for Audio Deepfake Detection](https://www.pindrop.com/blog/unveiling-our-latest-innovation-pindrop-pulse-for-audio-deepfake-detection)
* [Data, privacy, and security for Azure OpenAI Service](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy)
* [The Prompt: Bringing risk management and data governance to your gen AI models](https://google.smh.re/33Q3)
* [Google Online Security Blog: AI-Powered Fuzzing: Breaking the Bug Hunting Barrier](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)
* [Microsoft Vulnerability Severity Classification for Artificial Intelligence and Machine Learning Systems](https://www.microsoft.com/en-us/msrc/aibugbar)
* [Securing LLM Systems Against Prompt Injection | NVIDIA Technical Blog](https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/)
* [NeMo Guardrails an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.](https://github.com/NVIDIA/NeMo-Guardrails/tree/main)
* [Model evaluation for extreme risks](https://arxiv.org/pdf/2305.15324.pdf)
* [Responsible AI at Google Research: Adversarial testing for generative AI safety](https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html)
* [LLM Safety Leaderboard - a Hugging Face Space by AI-Secure](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard)
* [https://github.com/corca-ai/awesome-llm-security/](https://github.com/corca-ai/awesome-llm-security/)
* [A principled approach to evolving choice and control for web content](https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/)
* [robots exclusion protocol](https://www.rfc-editor.org/rfc/rfc9309.html)
* [OpenAI identifies its GPTBot web crawler so you can block it • The Register](https://www.theregister.com/2023/08/08/openai_scraping_software/)
* [The AI Attack Surface Map v1.0](https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/)
* [ML safety course](https://course.mlsafety.org/)
* [GitHub - protectai/rebuff: LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
* [This new data poisoning tool lets artists fight back against generative AI | MIT Technology Review](https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/)
* [How to keep your art out of AI generators - The Verge](https://www.theverge.com/24063327/ai-art-protect-images-copyright-generators)
* [Facebook and Instagram to label all fake AI images - BBC News](https://www.bbc.co.uk/news/technology-68215619)
* [Trust & Safety - Meta Llama](https://ai.meta.com/llama/purple-llama/)
* [AI Incident Database](https://incidentdatabase.ai/)
* [AVID Documentation](https://avidml.gitbook.io/doc/)
* [LLM Vulnerability scanner Integrates with avid](https://avidml.org/blog/garak-integration/)  [GitHub - leondz/garak: LLM vulnerability scanner](https://github.com/leondz/garak)
* [AI Vulnerability Database](https://avidml.org/)
* [MITRE | ATLAS™](https://atlas.mitre.org/)
* [GitHub - protectai/llm-guard: The Security Toolkit for LLM Interactions](https://github.com/laiyer-ai/llm-guard)
[Failure Modes in Machine Learning | Microsoft Learn](https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning)
[The AI Attack Surface Map v1.0](https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/)
* [15 Open Source Responsible AI Toolkits and Projects to Use Today](https://opendatascience.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today/)
* [AI has a privacy problem, but these techniques could fix it | VentureBeat](https://venturebeat.com/ai/ai-has-a-privacy-problem-but-these-techniques-could-fix-it/)