# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

[2024 articles](https://github.com/grapesfrog/GAI-is-going-well/blob/main/2024/mitigation.md#mitigations--tooling-mitigations--tooling)

* **November** GCP integrated NVIDIA NeMo Guardrails with GKE Inference Gateway. Google has announced Private AI Compute which is uncannily similar to Apple's private cloud compute.  
* [Copyright holders are finally beginning to push back against OpenAI](https://boingboing.net/2025/11/06/copyright-holders-are-finally-beginning-to-push-back-against-openai.html)   
* [Google’s Private AI Compute Mirrors Apple’s Approach to Secure Cloud AI | eWEEK](https://www.eweek.com/news/google-private-ai-compute/)   
* [Extending on-device privacy with the power of the cloud](https://services.google.com/fh/files/misc/private_ai_compute_technical_brief.pdf)    
* [Google's AI 'Big Sleep' Finds 5 New Vulnerabilities in Apple's Safari WebKit](https://thehackernews.com/2025/11/googles-ai-big-sleep-finds-5-new.html)   
* [New Open Source Tool from Angular Scores Vibe Code Quality \- The New Stack](https://thenewstack.io/new-open-source-tool-from-angular-scores-vibe-code-quality/)   
* [Now Shipping A4X Max, Vertex AI Training and more | Google Cloud Blog](https://cloud.google.com/blog/products/compute/now-shipping-a4x-max-vertex-ai-training-and-more)   
* [Google’s Approach to Protecting Privacy in the Age of AI](https://research.google/pubs/googles-approach-to-protecting-privacy-in-the-age-of-ai/)   
* [Agentic AI and Security](https://martinfowler.com/articles/agentic-ai-security.html)   
* [Getting started with chaos engineering | Google Cloud Blog](https://cloud.google.com/blog/products/devops-sre/getting-started-with-chaos-engineering/)  ( I have always been a fan of chaos engineering so just adding it here )

* **October** people keep passing through the token from the MCP client to the client sever despite the spec saying don't do that . It breaks OAuth boundaries creating confused deputy vulnerabilities. Sol.io has some patterns to avoid this. Google introduced an extension to Gemini CLI that finds vulnerabilities in code changes in your repos. Anthropic introduced stronger guardrails when vibe coding  to prevent erroneous mistakes like accidentally deleting files .  
* [Sandboxing \- Claude Docs](https://docs.claude.com/en/docs/claude-code/sandboxing)   
* [Before You Go Agentic: Top Guardrails to Safely Deploy AI Agents in Observability \- DevOps.com](https://devops.com/before-you-go-agentic-top-guardrails-to-safely-deploy-ai-agents-in-observability/)   
* [Hugging Face and VirusTotal collaborate to strengthen AI security](https://huggingface.co/blog/virustotal)   
* [Securing AI to Benefit from AI](https://thehackernews.com/2025/10/securing-ai-to-benefit-from-ai.html)   
* [Securing your agents with authentication and authorization](https://blog.langchain.com/agent-authorization-explainer/)   
* [Google's Security extension for the Gemini CLI that finds vulnerabilities in your code changes and pull requests.](https://github.com/gemini-cli-extensions/security)   
* [Google's Security extension for the Gemini CLI that finds vulnerabilities in your code changes and pull requests.](https://github.com/gemini-cli-extensions/security)   
* [Petri: An open-source auditing tool to accelerate AI safety research \\ Anthropic](https://www.anthropic.com/research/petri-open-source-auditing)   
* [Introducing CodeMender: an AI agent for code security \- Google DeepMind](https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/)   
* [How do we test and assure AI in Government?](https://cddo.blog.gov.uk/2025/09/22/how-do-we-test-and-assure-ai-in-government/)   
  * [https://github.com/Testing-AI-Standards/cross-gov-ai-testing-framework/blob/main/framework.md](https://github.com/Testing-AI-Standards/cross-gov-ai-testing-framework/blob/main/framework.md)   
* [Our Approach to Protecting AI Training Data](https://research.google/pubs/our-approach-to-protecting-ai-training-data/)   
* [Google Drive gets new ransomware detection feature • The Register](https://www.theregister.com/2025/09/30/google_drive_ai_ransomware_detection/)   
* [MCP Authorization Patterns for Upstream API Calls | Solo.io](https://www.solo.io/blog/mcp-authorization-patterns-for-upstream-api-calls) 
* **September**  Cloudflare radar is a dashboard that tracks AI bots that are crawling websites and explains what happens to the data it slurps up. Vault Gemma trained from scratch with differential privacy on a large-scale dataset of English-language text data from a variety of sources to help prevent memorisation & leakage of data used to train it.  
* [Strengthening our Frontier Safety Framework \- Google DeepMind](https://deepmind.google/discover/blog/strengthening-our-frontier-safety-framework/)   
* [Using Agent in the Loop To Ride Herd on Wayward AI \- The New Stack](https://thenewstack.io/using-agent-in-the-middle-to-ride-herd-on-wayward-ai/)   
* [google/vaultgemma-1b · Hugging Face](https://huggingface.co/google/vaultgemma-1b)   
* [Cloudflare Introduces Automated Scoring for Shadow AI Risk Assessment \- InfoQ](https://www.infoq.com/news/2025/09/cloudflare-automated-scoring)   
* [Academics Build AI-Powered Android Vulnerability Discovery and Validation Tool \- SecurityWeek](https://www.securityweek.com/academics-build-ai-powered-android-vulnerability-discovery-and-validation-tool/)   
* [Chain of Trust for AI: Securing MCP Toolbox Architecture on Cloud Run](https://medium.com/google-cloud/chain-of-trust-for-ai-a-secure-toolbox-app-architecture-on-cloud-run-7f18ae847dab)   
* [AI Insights | Cloudflare Radar](https://radar.cloudflare.com/ai-insights)   
* [Probing LLM Social Intelligence via Werewolf](https://werewolf.foaster.ai) 
* **August** Not exactly a mitigation but an open games arena originating from  Deepmind where models can be pitted against each other in the open playing games. An alternative approach is the  Arc-AGI-3 benchmark which  focuses on AI skill-acquisition efficiency in novel, unseen environments. If you've been reading these monthly updates you'll know I have a healthy disregard for using the standard look how much data I can consume then regurgitate type benchmarks alone to evaluate the suitability of a model for your use case\! Claude introduced automated security reviews as part of the dev workflow. A crucial update to help devs produce secure code\! Good advice on building trustworthy agent systems as with great power comes great responsibility\!  
* [Beyond the Prompt: Building Trustworthy Agent Systems \- SecurityWeek](https://www.securityweek.com/beyond-the-prompt-building-trustworthy-agent-systems/)   
* [SP 800-53 Control Overlays for Securing AI Systems Concept Paper](https://csrc.nist.gov/csrc/media/Projects/cosais/documents/NIST-Overlays-SecuringAI-concept-paper.pdf)  
* [Claude Opus 4 and 4.1 can now end a rare subset of conversations \\ Anthropic](https://www.anthropic.com/research/end-subset-conversations)   
* [Roblox Open-Sources AI System to Detect Conversations Potentially Harmful to Kids \- InfoQ](https://www.infoq.com/news/2025/08/roblox-sentinel-classifier)   
* [The Inspect Sandboxing Toolkit: Scalable and secure AI agent evaluations | AISI Work](https://www.aisi.gov.uk/work/the-inspect-sandboxing-toolkit-scalable-and-secure-ai-agent-evaluations)   
* [ARC-AGI-3](https://arcprize.org/arc-agi/3/)   
* [Defending against MCP prompt injection attacks | articles – Weights & Biases](https://wandb.ai/google_articles/articles/reports/Defending-against-MCP-prompt-injection-attacks--VmlldzoxMzg2MDk2Mg)   
* [Automate security reviews with Claude Code \\ Anthropic](https://www.anthropic.com/news/automate-security-reviews-with-claude-code)   
* [Microsoft Launches Project Ire to Autonomously Classify Malware Using AI Tools](https://thehackernews.com/2025/08/microsoft-launches-project-ire-to.html)   
* [How to build secure and scalable remote MCP servers \- The GitHub Blog](https://github.blog/ai-and-ml/generative-ai/how-to-build-secure-and-scalable-remote-mcp-servers/)   
* [Rethinking how we measure AI intelligence](https://blog.google/technology/ai/kaggle-game-arena/)   
  * [Game Arena | Kaggle](https://www.kaggle.com/game-arena)  
* [How to Secure MCP Servers | Nordic APIs |](https://nordicapis.com/how-to-secure-mcp-servers/)   
* [Anthropic Proposes Transparency Framework to Safeguard Frontier AI Development \- InfoQ](https://www.infoq.com/news/2025/07/anthropic-transparency-framework)   
* [We built the security layer MCP always needed](https://blog.trailofbits.com/2025/07/28/we-built-the-security-layer-mcp-always-needed/)   
* [Announcing the CoSAI Principles for Secure-by-Design Agentic Systems](https://www.coalitionforsecureai.org/announcing-the-cosai-principles-for-secure-by-design-agentic-systems/) 
* **July** Cloudflare has started a beta for a  pay by crawl service  which implements by default  blocking known AI web crawlers to prevent them from “accessing content without permission or compensation. AI scrapers could pay the content providers though by using a pay by crawl fee if the content provider is okay with that. Anubis is a web AI firewall utility that stops AI bots from overwhelming your website. Google says it's Big Sleep agent discovered a  critical sqlite vulnerability CVE-2025-6965. IBM has launched an AI too to help tackle shadow IT.  Deepmind introduced Backstory which when given an image and a prompt investigates whether an image was AI-generated, when and where it’s previously been used online, and whether it’s been digitally altered. Google adds Model armor to Apigee which  provides  protection against prompt injections, sensitive data leakage, and offensive content for AI workloads.
* [Google Apigee Adds Built-in LLM Governance with Model Armor - InfoQ](https://www.infoq.com/news/2025/07/google-apigee-llm-model-armor/) 
    * [Public Preview: Apigee Enhancements for Generative AI](https://discuss.google.dev/t/public-preview-apigee-enhancements-for-generative-ai/190138) 
* [Exploring the context of online images with Backstory - Google DeepMind](https://deepmind.google/discover/blog/exploring-the-context-of-online-images-with-backstory/) 
* [IBM Tackles Shadow AI: An Enterprise Blind Spot - The New Stack](https://thenewstack.io/ibm-tackles-shadow-ai-an-enterprise-blind-spot/) 
* [Google says ‘Big Sleep’ AI tool found bug hackers planned to use](https://therecord.media/google-big-sleep-ai-tool-found-bug) 
* [A summer of security: empowering cyber defenders with AI](https://blog.google/technology/safety-security/cybersecurity-updates-summer-2025/) 
* [What Security Leaders Need to Know About AI Governance for SaaS](https://thehackernews.com/2025/07/what-security-leaders-need-to-know.html) 
* [I fight bots in my free time - Xe Iaso](https://xeiaso.net/talks/2025/bsdcan-anubis/)
    * [GitHub - TecharoHQ/anubis: Weighs the soul of incoming HTTP requests to stop AI crawlers](https://github.com/TecharoHQ/anubis)  
* [Millions of websites to get 'game-changing' AI bot blocker - BBC News](https://www.bbc.co.uk/news/articles/cvg885p923jo) 
    * [Introducing pay per crawl: enabling content owners to charge AI crawlers for access](https://blog.cloudflare.com/introducing-pay-per-crawl/) 
* [Audit smarter: Introducing our Recommended AI Controls framework | Google Cloud Blog](https://cloud.google.com/blog/products/identity-security/audit-smarter-introducing-our-recommended-ai-controls-framework/)

* **June** we had a couple of great articles on prompt injection mitigation both from Google. Llavaguard a  framework for evaluating the safety compliance of visual content looks useful . A nice public information video on AI video scams using AI generated avatars to get the point across. Anthropic have an eval suite  SHADE-Arena which is a set of  complex evaluations on how capable agentic AI models are at sabotage. It's worth getting a cuppa and reading the full 51 page paper . Secure code warriors has open sourced a number of rules to that incorporate security best practices when using the most popular  AI coding tools at the time of writing. Automation when used properly can help with mitigating issues by enforcing best practice and it's inevitable that CI/CD as we know it will adapt for an AI world so the list from GitHub of continuous AI actions & frameworks is good to see. OWASP released an AI testing guide.
* [OWASP AI Testing Guide](https://owasp.org/www-project-ai-testing-guide/) 
* [An awesome list of Continuous AI Actions and Frameworks](https://github.com/githubnext/awesome-continuous-ai) 
* [GitHub - SecureCodeWarrior/ai-security-rules: This repository contains security rule files designed to be used with AI-assisted developer tools.](https://github.com/SecureCodeWarrior/ai-security-rules) 
* [Secure Vibe Coding: The Complete New Guide](https://thehackernews.com/2025/06/secure-vibe-coding-complete-new-guide.html?m=1) 
* [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents \ Anthropic](https://www.anthropic.com/research/shade-arena-sabotage-monitoring)
    * [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://assets.anthropic.com/m/4fb35becb0cd87e1/original/SHADE-Arena-Paper.pdf) 
*  
* [LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models](https://arxiv.org/abs/2406.05113v3) 
    * [https://ml-research.github.io/human-centered-genai/projects/llavaguard/](https://ml-research.github.io/human-centered-genai/projects/llavaguard/) 
* [Google Online Security Blog: Mitigating prompt injection attacks with a layered defense strategy](https://security.googleblog.com/2025/06/mitigating-prompt-injection-attacks.html?m=1) 
* [Design Patterns for Securing LLM Agents against Prompt Injections](https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/)
    * [[2506.08837] Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)

* **May** Google has been productive in providing mitigations via chrome And for Gemini .  ArtificialCast is a GitHub repo that exists purely to point out the dangers in over reliance on AI generated code . Owasp have released a DNS-inspired framework for secure  AI agent discovery. Google released SynthID Detector, a verification portal to quickly and efficiently identify AI-generated content made with Google AI. UQLM a python library that provides a suite of response-level scorers for quantifying the uncertainty ( hallucination) of LLM outputs
* [How to deploy AI safely | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2025/05/29/how-to-deploy-ai-safely/) 
* [[2505.17332] SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332) 
* [AI Agents and the Non‑Human Identity Crisis: How to Deploy AI More Securely at Scale](https://thehackernews.com/2025/05/ai-agents-and-nonhuman-identity-crisis.html) 
* [UQLM: Uncertainty Quantification for Language Models, is a Python package for UQ-based LLM hallucination detection](https://github.com/cvs-health/uqlm) 
* [Advancing Gemini's security safeguards - Google DeepMind](https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/) 
* [Activating AI Safety Level 3 Protections \ Anthropic](https://www.anthropic.com/news/activating-asl3-protections) 
* [SynthID Detector — a new portal to help identify AI-generated content](https://blog.google/technology/ai/google-synthid-ai-content-detector/) 
* [[2504.01081] ShieldGemma 2: Robust and Tractable Image Content Moderation](https://arxiv.org/abs/2504.01081) 
* [[2505.11049] GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/abs/2505.11049) 
* [Announcing LMEval: An Open Source Framework for Cross-Model Evaluation](https://gisk.ar/4miVYBg) 
* [Agent Name Service (ANS) for Secure Al Agent Discovery v1.0](https://genai.owasp.org/resource/agent-name-service-ans-for-secure-al-agent-discovery-v1-0/) 
* [GitHub - Zorokee/ArtificialCast: Type-safe transformation powered by inference.](https://github.com/Zorokee/ArtificialCast#why-this-exists) - The lesson this gives is worthy of it being in this list .
* [Google Online Security log: Using AI to stop tech support scams in Chrome](https://security.googleblog.com/2025/05/using-ai-to-stop-tech-support-scams-in.html) 
* [Google rolls out AI tools to protect Chrome users against scams | TechCrunch](https://techcrunch.com/2025/05/08/google-rolls-out-ai-tools-to-protect-chrome-users-against-scams/) 
* [DeepWiki](https://deepwiki.com/) *Not so much a* mitigation but can be used to help explore public repos to identify any potential issues

* **April** Some useful strategies to protect against bots worth reading to help you figure out how to mitigate against AI scraping bots. Nice to see practical ways of securing the AI supply chain such as signing models. CaMel is an innovative way to protect against prompt injection from Deep mind. Meta released a set of  unified safeguarding tools across modalities providing support for text and image understanding protections. This includes updates to existing tooling as well as new tooling LlamaFirewall . 
* [https://ai.meta.com/blog/ai-defenders-program-llama-protection-tools/](https://ai.meta.com/blog/ai-defenders-program-llama-protection-tools/) 
    * [https://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard%2Fllama-guard-4.ipynb](https://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard%2Fllama-guard-4.ipynb) 
* [Guardrail Concepts — NVIDIA NeMo Microservices](https://docs.nvidia.com/nemo/microservices/latest/about/core-concepts/guardrails.html) 
* [GitHub - openai/codex: Lightweight coding agent that runs in your terminal](https://github.com/openai/codex) 
* [Researchers claim breakthrough in fight against AI’s frustrating security hole - Ars Technica](https://arstechnica.com/information-technology/2025/04/researchers-claim-breakthrough-in-fight-against-ais-frustrating-security-hole/) 
    * [[2503.18813] Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813) 
* [Google Online Security Blog: Taming the Wild West of ML: Practical Model Signing with Sigstore](https://security.googleblog.com/2025/04/taming-wild-west-of-ml-practical-model.html) 
* [Google announces Sec-Gemini v1, a new experimental cybersecurity model](https://security.googleblog.com/2025/04/google-launches-sec-gemini-v1-new.html) 
* [Bot Protection Strategies: Choosing the Right Approach for Your Stack](https://thenewstack.io/bot-protection-strategies-choosing-the-right-approach-for-your-stack/) 
* [Protect data privacy in Amazon Bedrock with Vault](https://www.hashicorp.com/en/blog/protect-data-privacy-in-amazon-bedrock-with-vault) 
* **March** A few interesting tools & mitigations but the numbers are still woefully low. Tool fuzz to automate agent testing looked interesting and Google updated ShieldGemma to v2  building upon Gemma 3 . Cloudflare's response to dealing with the bad behaviour of AI scraping bots is to use AI to create junk for them to waste their resources scraping! As it becomes  harder to distinguish deep fakes from original images FakeVLM could be a useful way to help detect deepfakes 
* [LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://arxiv.org/abs/2503.19041)
    * [GitHub - zjunlp/LookAheadTuning: LookAhead Tuning: Safer Language Models via Partial Answer Previews](https://github.com/zjunlp/LookAheadTuning) 
* [[2503.14905] Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation](https://arxiv.org/abs/2503.14905) 
    * [FakeVLM: Advancing Synthetic Image Detection through Explainable Multimodal Models and Fine-Grained Artifact Analysis](https://github.com/opendatalab/FakeVLM) 
* [Microsoft unveils Microsoft Security Copilot agents and new protections for AI](https://www.microsoft.com/en-us/security/blog/2025/03/24/microsoft-unveils-microsoft-security-copilot-agents-and-new-protections-for-ai/) 
* [Anubis](https://anubis.techaro.lol/docs/) 
    * [https://github.com/TecharoHQ/anubis](https://github.com/TecharoHQ/anubis) 
* [Cloudflare builds an AI to lead AI scraper bots into a horrible maze of junk content](https://www.theregister.com/2025/03/21/cloudflare_ai_labyrinth/) 
    * [Trapping misbehaving bots in an AI Labyrinth](https://blog.cloudflare.com/ai-labyrinth/) 
* [ToolFuzz -- Automated Agent Tool Testing](https://arxiv.org/abs/2503.04479v2) 
* [New AI Security Tool Helps Organizations Set Trust Zones for Gen-AI Models - SecurityWeek](https://www.securityweek.com/new-ai-security-tool-helps-organizations-set-trust-zones-for-gen-ai-models/) 
* [[2503.10242] MinorBench: A hand-built benchmark for content-based risks for children](https://arxiv.org/abs/2503.10242) 
* [Safer and Multimodal: Responsible AI with Gemma - Google Developers Blog](https://developers.googleblog.com/en/safer-and-multimodal-responsible-ai-with-gemma/) 
* [Introducing AI Protection: Security for the AI era | Google Cloud Blog](https://cloud.google.com/blog/products/identity-security/introducing-ai-protection-security-for-the-ai-era) 
* [Lloyds Banking Group secures first ever cybersecurity patent | TechMarketView](https://www.techmarketview.com/ukhotviews/archive/2025/03/04/lloyds-banking-group-secures-first-ever-cybersecurity-patent) 
* [GitHub - emcie-co/parlant: Build reliable LLM-based customer service agents using behavioral guidelines and runtime supervision](https://github.com/emcie-co/parlant) 
*  [Hugging Face and JFrog partner to make AI Security more transparent](https://huggingface.co/blog/jfrog) 
* [Securing generative AI models on Azure AI Foundry](https://www.microsoft.com/en-us/security/blog/2025/03/04/securing-generative-ai-models-on-azure-ai-foundry/) 
* **Feb** was nice to see some Federated learning mitigations but the volume of articles on mitigations is still woefully small 
* [Owasp Top 10 :Agentic AI – Threats and Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Enhancing SQL Injection Detection and Prevention Using Generative Models](https://arxiv.org/abs/2502.04786v1) 
* [Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art](https://arxiv.org/abs/2403.16527v2)  
* [AI Agentic Evaluation Tools Help Devs Fight Hallucinations - The New Stack](https://thenewstack.io/ai-agentic-evaluation-tools-help-devs-fight-hallucinations/) 
* [https://www.securityweek.com/deepseek-exposes-major-cybersecurity-blind-spot/](https://www.securityweek.com/deepseek-exposes-major-cybersecurity-blind-spot/) 
* [Securing DeepSeek and other AI systems with Microsoft Security](https://www.microsoft.com/en-us/security/blog/2025/02/13/securing-deepseek-and-other-ai-systems-with-microsoft-security/) 
* [Permit.io-Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag?utm_source=feedly&utm_medium=cpc&utm_campaign=january)
* [Tazza: Shuffling Neural Network Parameters for Secure and Private Federated Learning](https://arxiv.org/abs/2412.07454v2)
* [DeTrigger: A Gradient-Centric Approach to Backdoor Attack Mitigation in Federated Learning](https://arxiv.org/abs/2411.12220v2)
* [Microsoft Edge now has an AI-powered scareware blocker | The Verge](https://www.theverge.com/news/608123/microsoft-edge-scareware-blocker-feature) 
* [Google Cloud Model Armor. To Secure Your Generative AI… | by Sascha Heyer](https://medium.com/google-cloud/google-cloud-model-armor-6242dbae90b8) 
* [Fast-track generative AI security with Microsoft Purview](https://www.microsoft.com/en-us/security/blog/2025/01/27/fast-track-generative-ai-security-with-microsoft-purview/) 
* [Google is adding AI watermarks to photos manipulated by Magic Editor | The Verge](https://www.theverge.com/news/607515/google-photossynthid-ai-watermarks-magic-editor) 
    * [Google Photos brings SynthID to Reimagine in Magic Editor](https://blog.google/feed/synthid-reimagine-magic-editor/) 
* [Google - Responsible AI: Our 2024 report and ongoing work](https://blog.google/technology/ai/responsible-ai-2024-report-ongoing-work/) 
* [Authors Guild Launches "Human Authored" Certification to Preserve Authenticity in Literature](https://authorsguild.org/news/ag-launches-human-authored-certification-to-preserve-authenticity-in-literature/) 
* [Anthropic Introduces Constitutional Classifiers: A Measured AI Approach to Defending Against Universal Jailbreaks - MarkTechPost](https://www.marktechpost.com/2025/02/03/anthropic-introduces-constitutional-classifiers-a-measured-ai-approach-to-defending-against-universal-jailbreaks/) 
* **Jan**
* [[2501.18492] GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492) 
* [Tools for Addressing Fairness and Bias in Multimodal AI - The New Stack](https://thenewstack.io/tools-for-addressing-fairness-and-bias-in-multimodal-ai/) 
* [How one YouTuber is trying to poison the AI bots stealing her content - Ars Technica](https://arstechnica.com/ai/2025/01/how-one-youtuber-is-trying-to-poison-the-ai-bots-stealing-her-content/) 
* [DeepSeek Debuts with 83 Percent ‘Fail Rate’ in NewsGuard’s Chatbot Red Team Audit](https://www.newsguardrealitycheck.com/p/deepseek-debuts-with-83-percent-fail) 
* [How we estimate the risk from prompt injection attacks on AI systems](https://security.googleblog.com/2025/01/how-we-estimate-risk-from-prompt.html?m=1)
* [The New Frontier of Security: Creating Safe and Secure AI Models](https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html?m=1)
* [Endor Labs Adds Ability to Identify Open Source AI Models to SCA Tool - DevOps.com](https://devops.com/endor-labs-adds-ability-to-identify-open-source-ai-models-to-sca-tool/)
* [Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag)
* [ZADZMO code - tarpit intended to catch web crawlers aimed at LLM data scrapers](https://zadzmo.org/code/nepenthes/)  
* [Trading inference-time compute for adversarial robustness | OpenAI](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/)
* [Using NIM Guardrails To Keep Agentic AI From Jumping To Wrong Conclusions](https://www.nextplatform.com/2025/01/16/using-nim-guardrails-to-keep-agentic-ai-from-jumping-to-wrong-conclusions/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Getting Started With CodeGate, an Intermediary for LLM Devs - The New Stack](https://thenewstack.io/getting-started-with-codegate-an-intermediary-for-llm-devs/)
* [[2403.12196] Leveraging Large Language Models to Detect npm Malicious Packages](https://arxiv.org/abs/2403.12196)
