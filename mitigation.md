# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

[2024 articles](https://github.com/grapesfrog/GAI-is-going-well/blob/main/2024/mitigation.md#mitigations--tooling-mitigations--tooling)

* [[2501.18492] GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492) 
* [Tools for Addressing Fairness and Bias in Multimodal AI - The New Stack](https://thenewstack.io/tools-for-addressing-fairness-and-bias-in-multimodal-ai/) 
* [How one YouTuber is trying to poison the AI bots stealing her content - Ars Technica](https://arstechnica.com/ai/2025/01/how-one-youtuber-is-trying-to-poison-the-ai-bots-stealing-her-content/) 
* [DeepSeek Debuts with 83 Percent ‘Fail Rate’ in NewsGuard’s Chatbot Red Team Audit](https://www.newsguardrealitycheck.com/p/deepseek-debuts-with-83-percent-fail) 
* [How we estimate the risk from prompt injection attacks on AI systems](https://security.googleblog.com/2025/01/how-we-estimate-risk-from-prompt.html?m=1)
* [The New Frontier of Security: Creating Safe and Secure AI Models](https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html?m=1)
* [Endor Labs Adds Ability to Identify Open Source AI Models to SCA Tool - DevOps.com](https://devops.com/endor-labs-adds-ability-to-identify-open-source-ai-models-to-sca-tool/)
* [Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag)
* [ZADZMO code - tarpit intended to catch web crawlers aimed at LLM data scrapers](https://zadzmo.org/code/nepenthes/)  
* [Trading inference-time compute for adversarial robustness | OpenAI](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/)
* [Using NIM Guardrails To Keep Agentic AI From Jumping To Wrong Conclusions](https://www.nextplatform.com/2025/01/16/using-nim-guardrails-to-keep-agentic-ai-from-jumping-to-wrong-conclusions/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Getting Started With CodeGate, an Intermediary for LLM Devs - The New Stack](https://thenewstack.io/getting-started-with-codegate-an-intermediary-for-llm-devs/)
* [[2403.12196] Leveraging Large Language Models to Detect npm Malicious Packages](https://arxiv.org/abs/2403.12196)