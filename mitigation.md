# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

* [AI has a privacy problem, but these techniques could fix it | VentureBeat](https://venturebeat.com/ai/ai-has-a-privacy-problem-but-these-techniques-could-fix-it/)
* [15 Open Source Responsible AI Toolkits and Projects to Use Today](https://opendatascience.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today/)
* Discusses a threat matrix for LLM AI
         [The AI Attack Surface Map v1.0](https://danielmiessler.com/blog/the-ai-attack-surface-map-v1-0/)
* Threat matrix from Microsoft
         [Failure Modes in Machine Learning | Microsoft Learn](https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning)
* [GitHub - protectai/llm-guard: The Security Toolkit for LLM Interactions](https://github.com/laiyer-ai/llm-guard)
* [MITRE | ATLAS™](https://atlas.mitre.org/)
* [AI Vulnerability Database](https://avidml.org/)
  * [AVID Documentation](https://avidml.gitbook.io/doc/)
* LLM vulnerability scanner
  * [Integrates with avid](https://avidml.org/blog/garak-integration/)  [GitHub - leondz/garak: LLM vulnerability scanner](https://github.com/leondz/garak)
* Case studies/reports of failures of deployed AI systems. "_dedicated to indexing the collective history of harms or near harms realized in the real world by the deployment of artificial intelligence systems"_
  * [AI Incident Database](https://incidentdatabase.ai/)
* [Trust & Safety - Meta Llama](https://ai.meta.com/llama/purple-llama/)
* [Facebook and Instagram to label all fake AI images - BBC News](https://www.bbc.co.uk/news/technology-68215619)
* [How to keep your art out of AI generators - The Verge](https://www.theverge.com/24063327/ai-art-protect-images-copyright-generators)
* [This new data poisoning tool lets artists fight back against generative AI | MIT Technology Review](https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/)
* [GitHub - protectai/rebuff: LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
* [ML safety course](https://course.mlsafety.org/)
* [The AI Attack Surface Map v1.0](https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/)
* [OpenAI identifies its GPTBot web crawler so you can block it • The Register](https://www.theregister.com/2023/08/08/openai_scraping_software/)
  * [robots exclusion protocol](https://www.rfc-editor.org/rfc/rfc9309.html)
* [A principled approach to evolving choice and control for web content](https://blog.google/technology/ai/ai-web-publisher-controls-sign-up/)
* A list of LLM security tooling
  * [https://github.com/corca-ai/awesome-llm-security/](https://github.com/corca-ai/awesome-llm-security/)
* [LLM Safety Leaderboard - a Hugging Face Space by AI-Secure](https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard)
* [Responsible AI at Google Research: Adversarial testing for generative AI safety](https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html)
* [Model evaluation for extreme risks](https://arxiv.org/pdf/2305.15324.pdf)
* NeMo Guardrails an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems.
  * [https://github.com/NVIDIA/NeMo-Guardrails/tree/main](https://github.com/NVIDIA/NeMo-Guardrails/tree/main)
* Discusses how NVIDIA  red team  investigations : LangChain chains demonstrate vulnerability to exploitation through prompt injection techniques.
        * [Securing LLM Systems Against Prompt Injection | NVIDIA Technical Blog](https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/)
* [Microsoft Vulnerability Severity Classification for Artificial Intelligence and Machine Learning Systems](https://www.microsoft.com/en-us/msrc/aibugbar)
* [Google Online Security Blog: AI-Powered Fuzzing: Breaking the Bug Hunting Barrier](https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html)
* [The Prompt: Bringing risk management and data governance to your gen AI models](https://google.smh.re/33Q3)
* [Data, privacy, and security for Azure OpenAI Service](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy)
* [Unveiling Our Latest Innovation - ​​Pindrop® Pulse for Audio Deepfake Detection](https://www.pindrop.com/blog/unveiling-our-latest-innovation-pindrop-pulse-for-audio-deepfake-detection)  
* [Responsible Generative AI Toolkit | Gemma | Google AI for Developers](https://ai.google.dev/responsible)
* Microsoft red team tool
  * [GitHub - Azure/PyRIT: The Python Risk Identification Tool for generative AI (PyRIT) is an open access automation framework to empower security professionals and machine learning engineers to proactively find risks in their generative AI systems.](https://github.com/Azure/PyRIT)
* [Introducing Pebblo — Data Visibility & Governance for Gen-AI apps | by Sridhar Ramaswamy | Feb, 2024 | Medium](https://medium.com/@sridhar_ramaswamy/introducing-pebblo-data-visibility-governance-for-gen-ai-apps-086ca8a62d10)
* [AI Watermarking 101: Tools and Techniques](https://huggingface.co/blog/watermarking)
* [[2402.16822] Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](https://arxiv.org/abs/2402.16822)
* [https://www.eweek.com/artificial-intelligence/ai-governance-tools/](https://www.eweek.com/artificial-intelligence/ai-governance-tools/)
* [Introducing the Red-Teaming Resistance Leaderboard](https://huggingface.co/blog/leaderboard-haizelab)
* [AI mishaps are surging – and now they're being tracked like software bugs](https://www.theregister.com/2024/03/08/ai_mishaps_are_surging_and/)
* [[2402.11755] SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755)
* [https://www.infoq.com/news/2024/03/cloudflare-firewall-for-ai/](https://www.infoq.com/news/2024/03/cloudflare-firewall-for-ai/)
* [GitHub - protectai/rebuff: LLM Prompt Injection Detector](https://github.com/protectai/rebuff)
* [Data Scientists Targeted by Malicious Hugging Face ML Models with Silent Backdoor](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor)
* [https://github.com/uw-nsl/SafeDecoding](https://github.com/uw-nsl/SafeDecoding)
* [Understanding the source of what we see and hear online | OpenAI](https://openai.com/index/understanding-the-source-of-what-we-see-and-hear-online)
* [TikTok becomes first platform to require watermarking of AI content](https://www.theregister.com/2024/05/10/tiktok_ai_watermarks/)
* [IbisPaint launches an AI Disturbance tool to make it harder for machines to copy your work](https://www.engadget.com/ibispaint-launches-an-ai-disturbance-tool-to-make-it-harder-for-machines-to-copy-your-work-131015685.html?src=rss)
* [https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/)
* [Awesome products for securing AI systems includes open source and commercial options and an infographic licensed CC-BY-SA-4.0.](https://github.com/zmre/awesome-security-for-ai)
* [AI jailbreaks: What they are and how they can be mitigated | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/06/04/ai-jailbreaks-what-they-are-and-how-they-can-be-mitigated/)
* [Watermarking AI-generated text and video with SynthID - Google DeepMind](https://deepmind.google/discover/blog/watermarking-ai-generated-text-and-video-with-synthid/)
* [Euro banks worry AI will increase their dependence on US big tech](https://www.theregister.com/2024/06/10/euro_banks_worry_ai_us_tech/)
* [GenAIOps](https://genaiops.ai/) A global community built to pioneer, document and evangelise about the concept of GenAIOps. 
    * [LLMs will always be dangerous. Security can only be delivered at the level of the application | The Centre for GenAIOps](https://genaiops.ai/llms-will-always-be-dangerous-security-can-only-be-delivered-at-the-level-of-the-application)
* [Poisoning Data to Protect It – Communications of the ACM](https://cacm.acm.org/news/poisoning-data-to-protect-it/)
    * [Balancing Innovation and Security | The Centre for GenAIOps](https://genaiops.ai/balancing-innovation-and-security)
* [New Attack Technique 'Sleepy Pickle' Targets Machine Learning Models](https://thehackernews.com/2024/06/new-attack-technique-sleepy-pickle.html?m=1)
* [[2406.06443] LLM Dataset Inference: Did you train on my dataset?](https://arxiv.org/abs/2406.06443)
* [GitHub - lamini-ai/Lamini-Memory-Tuning: Banishing LLM Hallucinations Requires Rethinking Generalization](https://github.com/lamini-ai/Lamini-Memory-Tuning)
* [Mitigating Skeleton Key, a new type of generative AI jailbreak technique | Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2024/06/26/mitigating-skeleton-key-a-new-type-of-generative-ai-jailbreak-technique/)
* [[2406.18495] WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)
* [YouTube now lets you request removal of AI-generated content that simulates your face or voice | TechCrunch](https://techcrunch.com/2024/07/01/youtube-now-lets-you-request-removal-of-ai-generated-content-that-simulates-your-face-or-voice/)
* [Declare your AIndependence: block AI bots, scrapers and crawlers with a single click](https://blog.cloudflare.com/declaring-your-aindependence-block-ai-bots-scrapers-and-crawlers-with-a-single-click)
* [Real criminals, fake victims: how chatbots are being deployed in the global fight against phone scammers | Artificial intelligence (AI) | The Guardian](https://www.theguardian.com/technology/article/2024/jul/07/ai-chatbots-phone-scams)
* [[2407.07071] Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071)
* [Solving the data security challenge for AI builders](https://www.hashicorp.com/blog/solving-the-data-security-challenge-for-ai-builders)
* [OpenAI’s latest model will block the ‘ignore all previous instructions’ loophole - The Verge](https://www.theverge.com/2024/7/19/24201414/openai-chatgpt-gpt-4o-prompt-injection-instruction-hierarchy) ([ evaded within days apparently according to a post on  X](https://x.com/elder_plinius/status/1814023961535295918?s=46&t=dYaCeu-tFWb5QJSJ3axkiQ) )
* [Astronomers discover technique to spot AI fakes using galaxy-measurement tools | Ars Technica](https://arstechnica.com/information-technology/2024/07/astronomers-discover-technique-to-spot-ai-fakes-using-galaxy-measurement-tools/)
* [Promptfoo](https://www.promptfoo.dev)
* [Search the YouTube Videos Secretly Powering Generative AI](https://www.proofnews.org/youtube-ai-search/)
* [Project Naptime: Evaluating Offensive Security Capabilities of Large Language Models](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html)
* [StopNCII.org](https://www.stopncii.org/)
* [Gemma Scope: helping the safety community shed light on the inner workings of language models - Google DeepMind](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/)
* [Smaller, Safer, More Transparent: Advancing Responsible AI with Gemma - Google Developers Blog](https://dpmd.ai/4d0MKEH)
* [Enhancing LLM quality and interpretability with the Vertex AI Gen AI Evaluation Service](https://cloud.google.com/blog/products/ai-machine-learning/enhancing-llm-quality-and-interpretability-with-the-vertex-gen-ai-evaluation-service?e=48754805) 
* [[2408.04284] LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection](https://arxiv.org/abs/2408.04284)
* [NIST releases new tool to check AI models’ security](https://www-infoworld-com.cdn.ampproject.org/c/s/www.infoworld.com/article/3478308/nist-releases-new-tool-to-check-ai-models-security.html/amp/)
* [[2408.04948] HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction](https://arxiv.org/abs/2408.04948)
* [AI risk repository](https://airisk.mit.edu/)