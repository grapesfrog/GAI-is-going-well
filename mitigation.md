# Categories

[In the wild](https://github.com/grapesfrog/GAI-is-going-well/blob/main/in-the-wild.md#in-the-wild-in-the-wild)

[Regulating AI/Advisories](https://github.com/grapesfrog/GAI-is-going-well/blob/main/regulate-ai.md#regulating-ai--advisories-regulating-ai-advisories)

[Opinions , Research & presentations ](https://github.com/grapesfrog/GAI-is-going-well/blob/main/opinion.md#opinions--research--presentations-opinions-research--presentations)

[Mitigations & tooling](https://github.com/grapesfrog/GAI-is-going-well/blob/main/mitigation.md#mitigations--tooling-mitigations--tooling)

## Mitigations & tooling {#mitigations-&-tooling}

[2024 articles](https://github.com/grapesfrog/GAI-is-going-well/blob/main/2024/mitigation.md#mitigations--tooling-mitigations--tooling)

* [Microsoft Edge now has an AI-powered scareware blocker | The Verge](https://www.theverge.com/news/608123/microsoft-edge-scareware-blocker-feature) 
* [Google Cloud Model Armor. To Secure Your Generative AI… | by Sascha Heyer](https://medium.com/google-cloud/google-cloud-model-armor-6242dbae90b8) 
* [Fast-track generative AI security with Microsoft Purview](https://www.microsoft.com/en-us/security/blog/2025/01/27/fast-track-generative-ai-security-with-microsoft-purview/) 
* [Google is adding AI watermarks to photos manipulated by Magic Editor | The Verge](https://www.theverge.com/news/607515/google-photossynthid-ai-watermarks-magic-editor) 
    * [Google Photos brings SynthID to Reimagine in Magic Editor](https://blog.google/feed/synthid-reimagine-magic-editor/) 
* [Google - Responsible AI: Our 2024 report and ongoing work](https://blog.google/technology/ai/responsible-ai-2024-report-ongoing-work/) 
* [Authors Guild Launches "Human Authored" Certification to Preserve Authenticity in Literature](https://authorsguild.org/news/ag-launches-human-authored-certification-to-preserve-authenticity-in-literature/) 
* [Anthropic Introduces Constitutional Classifiers: A Measured AI Approach to Defending Against Universal Jailbreaks - MarkTechPost](https://www.marktechpost.com/2025/02/03/anthropic-introduces-constitutional-classifiers-a-measured-ai-approach-to-defending-against-universal-jailbreaks/) 
* [[2501.18492] GuardReasoner: Towards Reasoning-based LLM Safeguards](https://arxiv.org/abs/2501.18492) 
* [Tools for Addressing Fairness and Bias in Multimodal AI - The New Stack](https://thenewstack.io/tools-for-addressing-fairness-and-bias-in-multimodal-ai/) 
* [How one YouTuber is trying to poison the AI bots stealing her content - Ars Technica](https://arstechnica.com/ai/2025/01/how-one-youtuber-is-trying-to-poison-the-ai-bots-stealing-her-content/) 
* [DeepSeek Debuts with 83 Percent ‘Fail Rate’ in NewsGuard’s Chatbot Red Team Audit](https://www.newsguardrealitycheck.com/p/deepseek-debuts-with-83-percent-fail) 
* [How we estimate the risk from prompt injection attacks on AI systems](https://security.googleblog.com/2025/01/how-we-estimate-risk-from-prompt.html?m=1)
* [The New Frontier of Security: Creating Safe and Secure AI Models](https://opensource.googleblog.com/2025/01/creating-safe-secure-ai-models.html?m=1)
* [Endor Labs Adds Ability to Identify Open Source AI Models to SCA Tool - DevOps.com](https://devops.com/endor-labs-adds-ability-to-identify-open-source-ai-models-to-sca-tool/)
* [Building AI Applications with Enterprise-Grade Security Using RAG and FGA](https://www.permit.blog/blog/building-ai-applications-with-enterprise-grade-security-using-fga-and-rag)
* [ZADZMO code - tarpit intended to catch web crawlers aimed at LLM data scrapers](https://zadzmo.org/code/nepenthes/)  
* [Trading inference-time compute for adversarial robustness | OpenAI](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/)
* [Using NIM Guardrails To Keep Agentic AI From Jumping To Wrong Conclusions](https://www.nextplatform.com/2025/01/16/using-nim-guardrails-to-keep-agentic-ai-from-jumping-to-wrong-conclusions/) 
* [GitHub - vectara/hallucination-leaderboard: Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents](https://github.com/vectara/hallucination-leaderboard) 
* [Getting Started With CodeGate, an Intermediary for LLM Devs - The New Stack](https://thenewstack.io/getting-started-with-codegate-an-intermediary-for-llm-devs/)
* [[2403.12196] Leveraging Large Language Models to Detect npm Malicious Packages](https://arxiv.org/abs/2403.12196)